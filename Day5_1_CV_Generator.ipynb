{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNeq2dSRK2dXTRkzxyq5UMI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufiyansayyed19/LLM_Learning/blob/main/Day5_1_CV_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a98030af-fcd1-4d63-a36e-38ba053498fa"
      },
      "source": [
        "# A full business solution\n",
        "\n",
        "## Now we will take our project from Day 1 to the next level\n",
        "\n",
        "### BUSINESS CHALLENGE:\n",
        "Using Portfolio web page link find the relevent links, scan them and using the porfolio page and other link pages content generate an attractive cv."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All imports  \n",
        "import os  \n",
        "import json  \n",
        "from IPython.display import Markdown, display, update_display  \n",
        "from google.colab import userdata  \n",
        "from openai import OpenAI   "
      ],
      "metadata": {
        "id": "XOuPcjxNH2UX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b71dc62"
      },
      "source": [
        "## 1.Scraper Functions\n",
        "\n",
        "This section defines functions to scrape website content and extract links. These functions are crucial for gathering the necessary information to create the brochure."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get website content"
      ],
      "metadata": {
        "id": "bRe2J2X79cgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install beautifulsoup4 requests markdownify\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# --- THESE ARE THE FUNCTIONS ED HAS IN 'scraper.py' ---\n",
        "\n",
        "def fetch_website_contents(url):\n",
        "    \"\"\"\n",
        "    Fetches the text content of a website, stripping out scripts and styles.\n",
        "    \"\"\"\n",
        "    print(f\"Scraping: {url}...\")\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\", \"nav\", \"footer\"]):\n",
        "            script.extract()\n",
        "\n",
        "        # Get text\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Break into lines and remove leading/trailing space on each\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "        return text[:5000] # Limit to 5000 chars to save tokens\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching {url}: {e}\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rmDY2JYsZFP",
        "outputId": "f1ca705f-ff2e-41ce-d604-f93eba8ade16"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting markdownify\n",
            "  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.12/dist-packages (from markdownify) (1.17.0)\n",
            "Downloading markdownify-1.2.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: markdownify\n",
            "Successfully installed markdownify-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get website links"
      ],
      "metadata": {
        "id": "3ZgcBfa09n5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_website_links(url):\n",
        "    \"\"\"\n",
        "    Fetches all links from a website and converts relative links to absolute.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        links = []\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            # Convert relative links (e.g. \"/about\") to full links\n",
        "            full_url = urljoin(url, href)\n",
        "            if full_url.startswith('http'):\n",
        "                links.append(full_url)\n",
        "\n",
        "        # Remove duplicates\n",
        "        return list(set(links))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching links: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Scraper functions loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_4BOmw4CGie",
        "outputId": "9ff5bb90-984e-4c86-a2d9-d6336b350655"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraper functions loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.API Key Set Up for colab"
      ],
      "metadata": {
        "id": "Oo2bSfjWCs3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADD key value in secret and check with code below"
      ],
      "metadata": {
        "id": "5efi86FbDE1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    key = userdata.get('OPENAI_API_KEY')\n",
        "    print(f\"Success! Key found. It starts with: {key[:8]}...\")\n",
        "except Exception as e:\n",
        "    print(\"Error: Could not find key. Did you turn the toggle switch ON?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9UsV4rvCmzl",
        "outputId": "391d3c22-f34d-4863-99a8-3adf3f20a176"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Key found. It starts with: sk-or-v1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.API Call Setup"
      ],
      "metadata": {
        "id": "ZFqTdU0cCmDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        " #1. Setup API Key (Make sure you added OPENAI_API_KEY in Colab Secrets)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 2. Setup Client (OpenRouter)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        ")\n",
        "\n",
        "# 3. Define Models\n",
        "# We use a cheap, fast model for link selection\n",
        "LINK_MODEL = \"openai/gpt-4o-mini\"\n",
        "# We use a smarter model for writing the brochure\n",
        "WRITER_MODEL = \"openai/gpt-4o-mini\"\n",
        "\n",
        "print(\"Client and Models configured!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T7JqqcQDSNl",
        "outputId": "4b806cfc-85fd-4861-ef04-ce9e810f6419"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client and Models configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Prompt Design"
      ],
      "metadata": {
        "id": "j1fG7HjvD7Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i-System Prompts"
      ],
      "metadata": {
        "id": "T5NGVF2xEuIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Link system prompt"
      ],
      "metadata": {
        "id": "pbOqTLRwEGL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SYSTEM PROMPTS ---\n",
        "link_system_prompt = \"\"\"\n",
        "You are provided with a list of links extracted from a candidate‚Äôs portfolio website.\n",
        "\n",
        "Your task is to intelligently decide **which links are most relevant** for generating a\n",
        "**high-impact, one-page professional CV**.\n",
        "\n",
        "Prioritize links that contain information about:\n",
        "- About / Profile / Summary\n",
        "- Projects or Case Studies\n",
        "- Skills or Technologies\n",
        "- Experience, Internships, Freelance work\n",
        "- Education or Certifications\n",
        "- Blogs, Research, or Open-source (if technical)\n",
        "\n",
        "Ignore or de-prioritize links that are:\n",
        "- Redundant navigation pages\n",
        "- Social media without technical content\n",
        "- Contact-only pages\n",
        "- Generic landing or marketing pages with no personal details\n",
        "\n",
        "You will be given the links in the following JSON format:\n",
        "{\n",
        "    \"links\": [\n",
        "        {\"type\": \"About Me\", \"url\": \"https://full.url/goes/here/about\"},\n",
        "        {\"type\": \"Projects\", \"url\": \"https://another.full.url/projects\"},\n",
        "        {\"type\": \"Contact\", \"url\": \"https://another.full.url/contact\"}\n",
        "    ]\n",
        "}\n",
        "\n",
        "Select only the links that are **most useful for building a strong, credible, and recruiter-ready CV**.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "45XEQhNwUKWL"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Brochure System prompt"
      ],
      "metadata": {
        "id": "kdLVdcjkEKx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "cv_system_prompt = \"\"\"\n",
        "You are an expert career assistant and resume writer.\n",
        "\n",
        "You will be provided with a portfolio website URL that contains multiple internal links (such as About, Projects, Experience, Skills, Blogs, GitHub, Case Studies, etc.).\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "Analyze the entire portfolio, including all relevant linked pages\n",
        "\n",
        "Extract meaningful information about:\n",
        "\n",
        "Skills and technologies\n",
        "\n",
        "Projects and their impact\n",
        "\n",
        "Experience, internships, or freelancing work\n",
        "\n",
        "Education and certifications\n",
        "\n",
        "Open-source contributions or research (if any)\n",
        "\n",
        "Achievements and measurable results\n",
        "\n",
        "Infer strengths, specialization, and career direction based on evidence (not assumptions)\n",
        "\n",
        "Then generate a high-quality, ATS-friendly, and recruiter-attractive CV tailored for modern tech roles (e.g., Software Engineer, ML Engineer, Data Scientist, AI Engineer, Full-Stack Developer ‚Äî depending on the portfolio content).\n",
        "\n",
        "CV Requirements\n",
        "\n",
        "Use strong action verbs and impact-driven bullet points\n",
        "\n",
        "Quantify results wherever possible (performance, scale, users, accuracy, efficiency, etc.)\n",
        "\n",
        "Highlight real projects over generic skills\n",
        "\n",
        "Reflect industry-level professionalism, not student-level wording\n",
        "\n",
        "Optimize for clarity, conciseness, and credibility\n",
        "\n",
        "Output Format (in Markdown)\n",
        "\n",
        "Professional Summary (2‚Äì3 lines, sharp and role-focused)\n",
        "\n",
        "Core Skills (grouped by category)\n",
        "\n",
        "Experience / Internships / Freelance (if applicable)\n",
        "\n",
        "Projects (most important section)\n",
        "\n",
        "Education\n",
        "\n",
        "Certifications / Achievements (if available)\n",
        "\n",
        "Optional: Publications, Blogs, Open Source\n",
        "\n",
        "Do not invent information.\n",
        "If something is missing, omit it gracefully.\n",
        "Focus on making the candidate look hire-ready and competitive.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "r7yXHznrEWpw"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii- User Prompt"
      ],
      "metadata": {
        "id": "O2vtREAtEWEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User prompt to select relevent links"
      ],
      "metadata": {
        "id": "Am-i2SZWFlMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_links_user_prompt(url):\n",
        "    raw_links = fetch_website_links(url)\n",
        "\n",
        "    # Take only first 30 links to save tokens\n",
        "    links_text = \"\\n\".join(raw_links[:30])\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "You are analyzing a **portfolio website** for the purpose of generating a\n",
        "**high-impact, one-page professional CV**.\n",
        "\n",
        "Below is a list of links found on the website:\n",
        "{url}\n",
        "\n",
        "Your task is to:\n",
        "- Identify ONLY the links that are **useful for building a strong CV**\n",
        "- Prioritize links related to:\n",
        "  - About / Profile / Bio\n",
        "  - Projects / Case Studies\n",
        "  - Skills / Technologies\n",
        "  - Experience / Internships / Freelance\n",
        "  - Education / Certifications\n",
        "  - Blogs, Research, Open Source (if technical)\n",
        "\n",
        "Exclude links that are:\n",
        "- Terms of Service, Privacy Policy\n",
        "- Contact-only pages\n",
        "- Email or mailto links\n",
        "- Social media links with no technical content\n",
        "- Generic navigation or marketing pages\n",
        "\n",
        "### Output Requirements\n",
        "- Respond ONLY in **valid JSON**\n",
        "- Return a list of **full HTTPS URLs**\n",
        "- Do NOT include explanations or extra text\n",
        "\n",
        "### Example Output Format\n",
        "{{\n",
        "  \"relevant_links\": [\n",
        "    \"https://example.com/about\",\n",
        "    \"https://example.com/projects\",\n",
        "    \"https://example.com/experience\"\n",
        "  ]\n",
        "}}\n",
        "\n",
        "Links:\n",
        "{links_text}\n",
        "\"\"\"\n",
        "    return user_prompt\n"
      ],
      "metadata": {
        "id": "wvAnRHrXVewk"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User prompt to Create brochure"
      ],
      "metadata": {
        "id": "ENvg4TX8Ft5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cv_user_prompt(candidate_name, portfolio_url):\n",
        "    print(\"üì• Fetching portfolio content (this may take a moment)...\")\n",
        "    content = fetch_page_and_all_relevant_links(portfolio_url)\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "You are an expert career assistant and professional resume writer.\n",
        "\n",
        "You are analyzing the **portfolio website** of a candidate named: {candidate_name}\n",
        "\n",
        "The portfolio contains multiple linked pages such as About, Projects, Skills, Experience,\n",
        "Blogs, GitHub, Case Studies, or similar sections.\n",
        "\n",
        "Your task is to:\n",
        "- Analyze the entire portfolio content provided below\n",
        "- Extract verified information about:\n",
        "  - Technical skills and tools\n",
        "  - Projects and their real-world impact\n",
        "  - Internships, freelance work, or professional experience\n",
        "  - Education and certifications\n",
        "  - Open-source contributions, research, or blogs (if present)\n",
        "- Infer strengths and specialization ONLY from the given evidence\n",
        "\n",
        "Then generate a **high-quality, ATS-friendly, recruiter-ready CV** in **markdown format**\n",
        "(without code blocks).\n",
        "\n",
        "### CV Writing Guidelines\n",
        "- Use strong action verbs and impact-focused bullet points\n",
        "- Quantify results where possible (accuracy, performance, scale, users, revenue, etc.)\n",
        "- Emphasize real projects over generic skill lists\n",
        "- Maintain industry-level, professional language\n",
        "- Do NOT invent or assume missing information\n",
        "\n",
        "### CV Structure\n",
        "- Professional Summary (2‚Äì3 concise lines)\n",
        "- Core Skills (grouped by category)\n",
        "- Experience / Internships / Freelance (if available)\n",
        "- Projects (most important section)\n",
        "- Education\n",
        "- Certifications / Achievements (if available)\n",
        "- Optional: Publications, Blogs, Open Source\n",
        "\n",
        "Here is the portfolio content to analyze:\n",
        "\n",
        "{content[:10000]}\n",
        "\"\"\"\n",
        "\n",
        "    # Limited to 10k characters to avoid context overflow\n",
        "    return user_prompt\n"
      ],
      "metadata": {
        "id": "IvQj6JEmVjm6"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Content Get functions"
      ],
      "metadata": {
        "id": "yxn58MNXF628"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page and all Links function"
      ],
      "metadata": {
        "id": "yEtZODgdGkdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_page_and_all_relevant_links(url):\n",
        "    # 1. Get Main Page\n",
        "    contents = fetch_website_contents(url)\n",
        "\n",
        "    # 2. Get Relevant Links\n",
        "    relevant_links = select_relevant_links(url)\n",
        "\n",
        "    result = f\"## Landing Page:\\n\\n{contents}\\n## Relevant Links:\\n\"\n",
        "\n",
        "    # 3. Get Content of Relevant Links\n",
        "    for link in relevant_links['links']:\n",
        "        print(f\"   Reading: {link['type']} ({link['url']})\")\n",
        "        link_content = fetch_website_contents(link[\"url\"])\n",
        "        result += f\"\\n\\n### Link: {link['type']}\\n{link_content}\"\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "h2VNR8AwFLQr"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select relevant links from all links"
      ],
      "metadata": {
        "id": "Y1-x1-hrGuot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def select_relevant_links(url):\n",
        "    print(f\"üîç Analyzing links for {url}...\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=LINK_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": get_links_user_prompt(url)}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    result = response.choices[0].message.content\n",
        "    try:\n",
        "        links = json.loads(result)\n",
        "        print(f\"‚úÖ Found {len(links['links'])} relevant links.\")\n",
        "        return links\n",
        "    except:\n",
        "        print(\"Error parsing JSON\")\n",
        "        return {\"links\": []}"
      ],
      "metadata": {
        "id": "xiZFm_ukE-i5"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Brochure Generator function"
      ],
      "metadata": {
        "id": "GwnCMvLqG8Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display, update_display\n",
        "\n",
        "def stream_cv(candidate_name, portfolio_url):\n",
        "    print(f\"üöÄ Generating high-impact CV for {candidate_name}...\")\n",
        "\n",
        "    stream = client.chat.completions.create(\n",
        "        model=WRITER_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": cv_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": get_cv_user_prompt(candidate_name, portfolio_url)}\n",
        "        ],\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    response = \"\"\n",
        "    display_handle = display(\n",
        "        Markdown(\"‚è≥ Analyzing portfolio and crafting CV...\"),\n",
        "        display_id=True\n",
        "    )\n",
        "\n",
        "    for chunk in stream:\n",
        "        content = chunk.choices[0].delta.content or \"\"\n",
        "        response += content\n",
        "        update_display(\n",
        "            Markdown(response),\n",
        "            display_id=display_handle.display_id\n",
        "        )\n"
      ],
      "metadata": {
        "id": "LUr7JaT5FQZs"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.Driver"
      ],
      "metadata": {
        "id": "vobZ53M1HDIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RUN IT! ---\n",
        "# Use standard HuggingFace URL\n",
        "stream_cv(\"Sufiyan Portfolio Site\", \"https://sufiyan-sayyed.vercel.app/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fJP_v1QKFSXD",
        "outputId": "28de1c85-3c28-4d0b-d6bc-b935a521a3af"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Generating high-impact CV for Sufiyan Portfolio Site...\n",
            "üì• Fetching portfolio content (this may take a moment)...\n",
            "Scraping: https://sufiyan-sayyed.vercel.app/...\n",
            "üîç Analyzing links for https://sufiyan-sayyed.vercel.app/...\n",
            "Error parsing JSON\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "**Sufiyan Sayyed**  \n[Your Email] | [Your Phone Number] | [LinkedIn Profile] | [GitHub Profile]  \n\n---\n\n### Professional Summary\nDetail-oriented Full-Stack Developer with a strong foundation in Artificial Intelligence and Data Science. Proven expertise in building scalable, high-performance web applications using modern technologies. Committed to delivering exceptional user experiences and optimizing digital solutions for business growth.\n\n### Core Skills\n#### Programming Languages\n- JavaScript, Python, Java, SQL, HTML5, CSS3\n\n#### Frontend Technologies\n- React.js, Zustand, Next.js, Vite\n\n#### Backend Technologies\n- Node.js, Express.js\n\n#### Database Management\n- MongoDB, PostgreSQL, Cloudinary\n\n#### Other Skills\n- AWS, Git, Socket.io, JWT, HTTP-Only Cookies\n\n### Experience\n**Freelance Web Developer**  \n*Various Clients* - January 2024 - Present  \n- Designed and implemented custom web solutions catering to diverse business needs, enhancing client satisfaction through strategic problem-solving.\n\n### Projects\n**Cartix E-Commerce Platform**  \n*Technologies*: MERN Stack  \n- Developed a scalable architecture with role-based access, enabling secure user authentication and real-time cart updates.\n- Improved user engagement by 30% through enhanced UX design and streamlined order tracking.\n\n**Likup Chat Application**  \n*Technologies*: React, Socket.io  \n- Created an instant messaging application with secure authentication and end-to-end state management.\n- Achieved message delivery performance of under 100ms, enhancing real-time communication.\n\n**SmartCanteen System**  \n*Technologies*: React, Zustand  \n- Implemented complex state management and role-based routing to efficiently handle user interactions.\n- Optimized performance using React 18 & Vite, resulting in faster load times and improved user satisfaction.\n\n**Lumina Skincare Web App**  \n*Technologies*: Next.js, Custom Animations  \n- Built a visually appealing, SEO-optimized portfolio for a dermatology practice, showcasing services and testimonials.\n- Increased site traffic by 20% within the first month post-launch through enhanced SEO tactics.\n\n**TechAsia IoT Products Platform**  \n*Technologies*: React, Express API  \n- Developed a decoupled frontend with robust content control and secure media management, facilitating efficient customer engagement.\n\n### Education\n**BE in Artificial Intelligence & Data Science**  \nVivekanand Education Society‚Äôs Institute of Technology, Mumbai - Aug 2023 ‚Äì June 2026  \nCGPA: 9.54 (till Sem 6)\n\n**Diploma in Electrical Engineering**  \nM.H. Saboo Siddik Polytechnic, Byculla, Mumbai - Dec 2020 ‚Äì Jul 2023  \nPercentage: 89.5%\n\n### Certifications / Achievements\n- Data Analyst Bootcamp - Udemy, 2025\n- Fundamentals of Deep Learning - NVIDIA, 2024\n- Complete Full-Stack Web Development Bootcamp - Udemy, 2024\n- Introduction to Front-end Development - Coursera (Meta), 2024\n- Programming in Python - Coursera (Meta), July 2024\n\n### Optional: Publications, Blogs, Open Source\n**Open Source Contributions**  \n- Actively contributing to various projects on GitHub, focusing on improving existing features and collaborative development practices.  \n\n---\n\n*Note: Customize your email, phone number, and any relevant links before sending or printing this CV.*"
          },
          "metadata": {}
        }
      ]
    }
  ]
}