{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNH35AOqHPZv7oDsyHEK+vc",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufiyansayyed19/LLM_Learning/blob/main/W2D1_playWithAPIs_and_Tokens.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calling Different APIs"
      ],
      "metadata": {
        "id": "ft5c4Q-_wQ5D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API key setup"
      ],
      "metadata": {
        "id": "ueT2cNNBwZRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "open_route_api_key = userdata.get(\"OPEN_ROUTE_API_KEY\")\n",
        "gemini_api_key = userdata.get(\"GEMINI_API_KEY\")"
      ],
      "metadata": {
        "id": "H9UsV4rvCmzl"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Keys"
      ],
      "metadata": {
        "id": "2Ui5Uma-4uzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if open_route_api_key:\n",
        "  print(\"open_route api key is good.\")\n",
        "else:\n",
        "  print(\"open_route api key not found.\")\n",
        "\n",
        "if gemini_api_key:\n",
        "  print(\"gemini_api_key is good.\")\n",
        "else:\n",
        "  print(\"gemini_api key not found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ItAESYLoPo5g",
        "outputId": "f5965556-39dd-484a-dbf6-c3a87ddeb317"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "open_route api key is good.\n",
            "gemini_api_key is good.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Url and base set up"
      ],
      "metadata": {
        "id": "tXw_9VsK4y21"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gemini_url = \"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
        "open_route_url = \"https://openrouter.ai/api/v1\""
      ],
      "metadata": {
        "id": "UULyfUEpQNRL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI"
      ],
      "metadata": {
        "id": "-K2XIPolTFqU"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "openroute = OpenAI(base_url=open_route_url,api_key=open_route_api_key)\n",
        "\n",
        "gemini = OpenAI(base_url=gemini_url, api_key=gemini_api_key)"
      ],
      "metadata": {
        "id": "JDYi5f_aTJ6D"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## User prompt"
      ],
      "metadata": {
        "id": "z2YHTMHt45vM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tell_a_joke = [\n",
        "    { \"role\": \"user\", \"content\":\"Tell a joke for a student on the journey to becoming an expert in LLM Engineering\",    }\n",
        "]"
      ],
      "metadata": {
        "id": "zLJp9tubQws-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## API call with markdown"
      ],
      "metadata": {
        "id": "pCsUVtV449sV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenRoute"
      ],
      "metadata": {
        "id": "J2LPzeCj5Biu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown,display\n",
        "\n",
        "response = openroute.chat.completions.create(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    messages=tell_a_joke\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "qPbhKdf0SFms",
        "outputId": "a9f81bfd-0e92-4f55-c01d-421eb9db5ac1"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why did the LLM engineer break up with their dataset?\n\nBecause it had too many bias issues and just couldn't keep things balanced!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Gemini"
      ],
      "metadata": {
        "id": "IeN5iedX5GMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = gemini.chat.completions.create(\n",
        "    model=\"gemini-2.5-flash\",\n",
        "    messages=tell_a_joke)\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "fVjcydzWSs7c",
        "outputId": "84c9d53c-2567-41b7-ba06-ba0e0ed581b3"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Okay, here's one for a budding LLM Engineer:\n\nA junior LLM Engineer excitedly tells their senior colleague, \"I finally got my model to *reliably* generate text exactly as I wanted!\"\n\nThe senior engineer raises an eyebrow. \"Oh really? What did you prompt it with?\"\n\nThe junior engineer beams, \"I simply wrote: 'Generate the word \"banana\".'\"\n\nThe senior engineer nods slowly. \"Ah, the early days of finding stability. Next, try 'Generate a six-paragraph, emotionally resonant short story about a talking banana, formatted as a JSON object, but ensure it *never* mentions the color yellow.'\"\n\nThe junior engineer's smile slowly fades. \"Oh... right.\"\n\n**Welcome to LLM Engineering!**"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Gemini Base code (alternative to openai - not commonly used)"
      ],
      "metadata": {
        "id": "WqsNI_Fb5lHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google import genai\n",
        "\n",
        "client = genai.Client()\n",
        "\n",
        "response = client.models.generate_content(\n",
        "    model=\"gemini-2.5-flash-lite\", contents=\"Describe the color Blue to someone who's never been able to see in 1 sentence\"\n",
        ")\n",
        "print(response.text)"
      ],
      "metadata": {
        "id": "OqFeeRVk1uum"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Antrophhic Base code (alternative to openai - not commonly used)"
      ],
      "metadata": {
        "id": "BBhOn_qB5r6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "\n",
        "client = Anthropic()\n",
        "\n",
        "response = client.messages.create(\n",
        "    model=\"claude-sonnet-4-5-20250929\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Describe the color Blue to someone who's never been able to see in 1 sentence\"}],\n",
        "    max_tokens=100\n",
        ")\n",
        "print(response.content[0].text)"
      ],
      "metadata": {
        "id": "DPdPETy89U5b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Langchain First look ( powerful but heavy)"
      ],
      "metadata": {
        "id": "PD1vSIEX6znf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-openai langchain-core langchain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWJ6kMej9EkB",
        "outputId": "e012a717-b4ec-42c9-b6f8-d13dd47f7f6f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-1.1.7-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: langchain-core in /usr/local/lib/python3.12/dist-packages (1.2.1)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (1.2.0)\n",
            "Collecting langchain-core\n",
            "  Downloading langchain_core-1.2.6-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: openai<3.0.0,>=1.109.1 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (2.12.0)\n",
            "Requirement already satisfied: tiktoken<1.0.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (1.33)\n",
            "Requirement already satisfied: langsmith<1.0.0,>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.4.59)\n",
            "Requirement already satisfied: packaging<26.0.0,>=23.2.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (25.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (2.12.3)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (6.0.3)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (9.1.2)\n",
            "Requirement already satisfied: typing-extensions<5.0.0,>=4.7.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (4.15.0)\n",
            "Requirement already satisfied: uuid-utils<1.0,>=0.12.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core) (0.12.0)\n",
            "Requirement already satisfied: langgraph<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langchain) (1.0.5)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core) (3.0.0)\n",
            "Requirement already satisfied: langgraph-checkpoint<4.0.0,>=2.1.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.0.1)\n",
            "Requirement already satisfied: langgraph-prebuilt<1.1.0,>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (1.0.5)\n",
            "Requirement already satisfied: langgraph-sdk<0.4.0,>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (0.3.0)\n",
            "Requirement already satisfied: xxhash>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from langgraph<1.1.0,>=1.0.2->langchain) (3.6.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (3.11.5)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith<1.0.0,>=0.3.45->langchain-core) (0.25.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.12.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai<3.0.0,>=1.109.1->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain-core) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken<1.0.0,>=0.7.0->langchain-openai) (2025.11.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai<3.0.0,>=1.109.1->langchain-openai) (3.11)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<1.0.0,>=0.3.45->langchain-core) (0.16.0)\n",
            "Requirement already satisfied: ormsgpack>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from langgraph-checkpoint<4.0.0,>=2.1.0->langgraph<1.1.0,>=1.0.2->langchain) (1.12.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith<1.0.0,>=0.3.45->langchain-core) (2.5.0)\n",
            "Downloading langchain_openai-1.1.7-py3-none-any.whl (84 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.8/84.8 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-1.2.6-py3-none-any.whl (489 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m489.1/489.1 kB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: langchain-core, langchain-openai\n",
            "  Attempting uninstall: langchain-core\n",
            "    Found existing installation: langchain-core 1.2.1\n",
            "    Uninstalling langchain-core-1.2.1:\n",
            "      Successfully uninstalled langchain-core-1.2.1\n",
            "Successfully installed langchain-core-1.2.6 langchain-openai-1.1.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "\n",
        "llm = ChatOpenAI(\n",
        "    model = \"openai/gpt-4o-mini\",\n",
        "    base_url = \"https://openrouter.ai/api/v1\",\n",
        "    api_key=  open_route_api_key\n",
        "\n",
        ")\n",
        "\n",
        "message = [HumanMessage(content=\"Tell me a llm joke\")]\n",
        "\n",
        "response =  llm.invoke(message)\n",
        "\n",
        "display(Markdown(response.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "lX-SeCh36ynz",
        "outputId": "b53fa017-210e-47b7-9661-16f5d0f24aec"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why did the large language model break up with its partner?\n\nBecause it just couldn‚Äôt stop generating misunderstandings!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  LiteLLM ( light weigth)"
      ],
      "metadata": {
        "id": "wY_6Bt5E2A9s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install LiteLLM\n",
        "!pip install litellm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzEGqq399nAv",
        "outputId": "76924277-5cf0-429e-af01-54bfd8d1a856"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting litellm\n",
            "  Downloading litellm-1.80.12-py3-none-any.whl.metadata (29 kB)\n",
            "Requirement already satisfied: aiohttp>=3.10 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.13.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from litellm) (8.3.1)\n",
            "Collecting fastuuid>=0.13.0 (from litellm)\n",
            "  Downloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: grpcio!=1.68.*,!=1.69.*,!=1.70.*,!=1.71.0,!=1.71.1,!=1.72.0,!=1.72.1,!=1.73.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.76.0)\n",
            "Requirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.28.1)\n",
            "Requirement already satisfied: importlib-metadata>=6.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (8.7.0)\n",
            "Requirement already satisfied: jinja2<4.0.0,>=3.1.2 in /usr/local/lib/python3.12/dist-packages (from litellm) (3.1.6)\n",
            "Requirement already satisfied: jsonschema<5.0.0,>=4.23.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (4.25.1)\n",
            "Requirement already satisfied: openai>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (2.12.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (2.12.3)\n",
            "Requirement already satisfied: python-dotenv>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (1.2.1)\n",
            "Requirement already satisfied: tiktoken>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from litellm) (0.12.0)\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (from litellm) (0.22.1)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (6.7.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.10->litellm) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions~=4.12 in /usr/local/lib/python3.12/dist-packages (from grpcio!=1.68.*,!=1.69.*,!=1.70.*,!=1.71.0,!=1.71.1,!=1.72.0,!=1.72.1,!=1.73.0,>=1.62.3->litellm) (4.15.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.23.0->litellm) (3.11)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.23.0->litellm) (0.16.0)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.12/dist-packages (from importlib-metadata>=6.8.0->litellm) (3.23.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2<4.0.0,>=3.1.2->litellm) (3.0.3)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema<5.0.0,>=4.23.0->litellm) (0.30.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.8.0->litellm) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from openai>=2.8.0->litellm) (0.12.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=2.8.0->litellm) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=2.8.0->litellm) (4.67.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.5.0->litellm) (0.4.2)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.7.0->litellm) (2.32.4)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers->litellm) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (6.0.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers->litellm) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (3.4.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.7.0->litellm) (2.5.0)\n",
            "Downloading litellm-1.80.12-py3-none-any.whl (11.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m65.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fastuuid-0.14.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (278 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fastuuid, litellm\n",
            "Successfully installed fastuuid-0.14.0 litellm-1.80.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from litellm import completion\n",
        "from IPython.display import Markdown,display\n",
        "# 1. Setup the Environment Variable (LiteLLM looks for this specific name)\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = open_route_api_key\n",
        "\n",
        "\n",
        "response = completion(\n",
        "    model=\"openrouter/openai/gpt-4o-mini\",\n",
        "    messages=tell_a_joke\n",
        ")\n",
        "\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 159
        },
        "id": "tcTAPw132VmL",
        "outputId": "63aaa1ba-cbff-4408-8aea-70c6bda32fcb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='Why did ...one, 'reasoning': None}), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...finish_reason': 'stop'}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "Why did the LLM refuse to play hide and seek?\n\nBecause good luck hiding when it can predict your next move!"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## We can see Tokens Sizes"
      ],
      "metadata": {
        "id": "AFoDqGTUDG4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
        "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
        "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cOrJpQEA01k",
        "outputId": "2fe8fb98-757e-41d6-91f9-94ec1ca9da12"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens: 24\n",
            "Output tokens: 24\n",
            "Total tokens: 48\n",
            "Total cost: 0.0018 cents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Now - let's use LiteLLM to illustrate a Pro-feature: prompt caching"
      ],
      "metadata": {
        "id": "X-JOGVNtA7O8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# 1. Download Hamlet from a public source (saving it to Colab)\n",
        "url = \"https://gist.githubusercontent.com/provpup/2fc41686eab7400b796b/raw/b575bd01a58494dfddc91e0143db631f36331463/hamlet.txt\"\n",
        "response = requests.get(url)\n",
        "\n",
        "with open(\"hamlet.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "    f.write(response.text)\n",
        "\n",
        "print(\"‚úÖ 'hamlet.txt' downloaded successfully!\")\n",
        "\n",
        "# 2. Now run your code\n",
        "with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    hamlet = f.read()\n",
        "\n",
        "# 3. Find the quote\n",
        "loc = hamlet.find(\"Speak, man\")\n",
        "\n",
        "# Safety check: if text isn't found in this specific version\n",
        "if loc != -1:\n",
        "    print(f\"\\nFound quote at index {loc}:\\n\")\n",
        "    print(hamlet[loc:loc+100])\n",
        "else:\n",
        "    print(\"Could not find the specific phrase in this version of the text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YffAX-AqA-XI",
        "outputId": "767af320-ff0c-406c-9fda-89b6754f83cd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ 'hamlet.txt' downloaded successfully!\n",
            "Could not find the specific phrase in this version of the text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "question = [{\"role\": \"user\", \"content\": \"In Hamlet, when Laertes asks 'Where is my father?' what is the reply?\"}]"
      ],
      "metadata": {
        "id": "qmsxsdV9Cs-0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(model=\"openrouter/openai/gpt-4o-mini\", messages=question)\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        },
        "id": "hyLk1nQMCvQY",
        "outputId": "9919ccf3-b86f-49cc-9024-8f781a9790f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/pydantic/main.py:464: UserWarning: Pydantic serializer warnings:\n",
            "  PydanticSerializationUnexpectedValue(Expected 10 fields but got 5: Expected `Message` - serialized value may not be as expected [field_name='message', input_value=Message(content='In Shake...one, 'reasoning': None}), input_type=Message])\n",
            "  PydanticSerializationUnexpectedValue(Expected `StreamingChoices` - serialized value may not be as expected [field_name='choices', input_value=Choices(finish_reason='st...finish_reason': 'stop'}), input_type=Choices])\n",
            "  return self.__pydantic_serializer__.to_python(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In Shakespeare's \"Hamlet,\" when Laertes asks \"Where is my father?\" he is speaking to King Claudius. The reply comes from Claudius, who tells Laertes that his father, Polonius, is dead. Claudius explains that Polonius was killed by Hamlet, which sets off further events in the play. This moment is crucial as it heightens Laertes' grief and desire for revenge against Hamlet."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
        "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
        "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fujeR3zsDB-2",
        "outputId": "63b8657f-f280-4a0c-ca05-41118c7073f9"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens: 25\n",
            "Output tokens: 90\n",
            "Total tokens: 115\n",
            "Total cost: 0.0058 cents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = completion(model=\"openrouter/openai/gpt-4o-mini\", messages=question)\n",
        "display(Markdown(response.choices[0].message.content))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "xYoJ7HqxEpIY",
        "outputId": "a0af4fef-6037-4bf2-ddcc-ec8af12b64d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "In Shakespeare's \"Hamlet,\" when Laertes asks \"Where is my father?\" he is met with a response from Gertrude that leads to the revelation of Polonius's death. Specifically, the reply comes from Queen Gertrude, who does not directly answer Laertes' question but instead indicates that Polonius is dead, leading to a dramatic progression in the play. Following this, Hamlet is discovered to be responsible for Polonius's death."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Input tokens: {response.usage.prompt_tokens}\")\n",
        "print(f\"Output tokens: {response.usage.completion_tokens}\")\n",
        "print(f\"Total tokens: {response.usage.total_tokens}\")\n",
        "print(f\"Total cost: {response._hidden_params[\"response_cost\"]*100:.4f} cents\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ahBKag2nEiqD",
        "outputId": "6dce5920-a3ec-433e-ac29-839a6d7ba5c3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input tokens: 25\n",
            "Output tokens: 94\n",
            "Total tokens: 119\n",
            "Total cost: 0.0060 cents\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "\n",
        "# 1. Setup Gemini Key (Fixed syntax)\n",
        "# We must get the key from userdata *before* setting the environment variable\n",
        "try:\n",
        "    # Get the key you saved in the Secrets (Key icon on the left)\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    if api_key:\n",
        "        os.environ['GEMINI_API_KEY'] = api_key\n",
        "        print(\"‚úÖ Gemini API Key loaded.\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è Warning: GEMINI_API_KEY is empty in Secrets.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error: Could not find GEMINI_API_KEY. Did you add it to the Secrets (Key icon)?\\n{e}\")\n",
        "\n",
        "# 2. Ensure Hamlet is loaded\n",
        "if 'book_content' not in globals():\n",
        "    try:\n",
        "        with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            book_content = f.read()\n",
        "    except FileNotFoundError:\n",
        "        # Fallback if file isn't there (downloads it)\n",
        "        import requests\n",
        "        url = \"https://gist.githubusercontent.com/provpup/2fc41686eab7400b796b/raw/b575bd01a58494dfddc91e0143db631f36331463/hamlet.txt\"\n",
        "        with open(\"hamlet.txt\", \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(requests.get(url).text)\n",
        "        with open(\"hamlet.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "            book_content = f.read()\n",
        "\n",
        "# 3. Construct the Message\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"Here is the book Hamlet:\\n{book_content}\\n\\nQuestion: What is the reply to 'Where is my father?'\"}\n",
        "]\n",
        "\n",
        "# 4. Use Gemini (Correct Syntax)\n",
        "# LiteLLM requires the \"gemini/\" prefix to know which provider to use.\n",
        "MODEL = \"gemini/gemini-2.5-flash\"  # <--- FIXED: Added 'gemini/' prefix\n",
        "\n",
        "print(f\"\\nü§ñ Testing with {MODEL}...\")\n",
        "\n",
        "# --- RUN 1 ---\n",
        "print(\"\\n--- üèÉ RUN 1 ---\")\n",
        "start = time.time()\n",
        "try:\n",
        "    response1 = completion(model=MODEL, messages=messages)\n",
        "    end = time.time()\n",
        "\n",
        "    print(f\"Time: {end - start:.2f}s\")\n",
        "    # Gemini Free Tier (API) usually returns $0 cost, but this checks just in case\n",
        "    cost = response1._hidden_params.get('response_cost', 0)\n",
        "    print(f\"Cost: ${cost:.6f}\")\n",
        "    print(f\"Answer: {response1.choices[0].message.content}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error calling Gemini: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jasCHjX_QUiv",
        "outputId": "b6bc8cf1-d969-4adf-ecb3-18c1fa76f0de"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Gemini API Key loaded.\n",
            "\n",
            "ü§ñ Testing with gemini/gemini-2.5-flash...\n",
            "\n",
            "--- üèÉ RUN 1 ---\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "Error calling Gemini: litellm.InternalServerError: litellm.InternalServerError: geminiException - {\n",
            "  \"error\": {\n",
            "    \"code\": 503,\n",
            "    \"message\": \"The model is overloaded. Please try again later.\",\n",
            "    \"status\": \"UNAVAILABLE\"\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update LiteLLM to the latest version (Crucial for Gemini 1.5/2.0 support)\n",
        "!pip install -Uq litellm requests\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# We use 'gemini/gemini-1.5-flash' which is the current stable standard.\n",
        "# If this fails, try 'gemini/gemini-pro' as a fallback.\n",
        "MODEL = \"gemini/gemini-1.5-flash\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: SETUP API KEY\n",
        "# ---------------------------------------------------------\n",
        "print(\"üîë Setting up API Key...\")\n",
        "try:\n",
        "    # Ensure you have added 'GEMINI_API_KEY' in the Colab Secrets (Key icon)\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    if api_key:\n",
        "        os.environ['GEMINI_API_KEY'] = api_key\n",
        "        print(\"‚úÖ Gemini API Key loaded.\")\n",
        "    else:\n",
        "        raise ValueError(\"Key is empty\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Could not find 'GEMINI_API_KEY'. Please check your Colab Secrets.\\n{e}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPARE THE BOOK (FIXED SOURCE)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\nüìñ Downloading Hamlet from Project Gutenberg...\")\n",
        "file_path = \"hamlet.txt\"\n",
        "\n",
        "# Use a stable Project Gutenberg URL\n",
        "url = \"https://www.gutenberg.org/cache/epub/1524/pg1524.txt\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status() # Raise error if download fails\n",
        "\n",
        "    # Save to file\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "    # Read back to verify\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        book_content = f.read()\n",
        "\n",
        "    # REMOVE GUTENBERG HEADER/FOOTER (Optional but cleaner)\n",
        "    # This keeps just the play text roughly\n",
        "    if \"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "        book_content = book_content.split(\"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[1]\n",
        "        if \"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "            book_content = book_content.split(\"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[0]\n",
        "\n",
        "    print(f\"‚úÖ Book loaded! Size: {len(book_content)} characters.\")\n",
        "\n",
        "    # ERROR CHECK: If size is small (<1000), the download failed.\n",
        "    if len(book_content) < 1000:\n",
        "        raise ValueError(\"The downloaded file is too small. The URL might be blocked.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to download book: {e}\")\n",
        "    # Fallback to a dummy text if download fails so code doesn't crash\n",
        "    book_content = \"To be, or not to be, that is the question.\" * 1000\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: RUN THE EXPERIMENT\n",
        "# ---------------------------------------------------------\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"Here is the book Hamlet:\\n{book_content[:50000]}\\n\\nQuestion: What is the reply to 'Where is my father?'\"}\n",
        "]\n",
        "\n",
        "print(f\"\\nü§ñ Starting Experiment with model: {MODEL}\")\n",
        "\n",
        "# --- RUN 1 ---\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"üèÉ RUN 1 (Cold Start)\")\n",
        "print(\"------------------------------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    response1 = completion(model=MODEL, messages=messages)\n",
        "    elapsed1 = time.time() - start_time\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed1:.2f} seconds\")\n",
        "    print(f\"üìù Answer: {response1.choices[0].message.content.strip()[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 1: {e}\")\n",
        "    print(\"üí° TIP: If you get a 404, check if 'Generative Language API' is enabled in your Google Cloud Console.\")\n",
        "\n",
        "# --- RUN 2 ---\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"üèÉ RUN 2 (Simulated Cache)\")\n",
        "print(\"------------------------------------------------\")\n",
        "start_time = time.time()\n",
        "\n",
        "try:\n",
        "    response2 = completion(model=MODEL, messages=messages)\n",
        "    elapsed2 = time.time() - start_time\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed2:.2f} seconds\")\n",
        "    if 'elapsed1' in locals():\n",
        "        print(f\"üöÄ Speed Difference: {elapsed1 - elapsed2:.2f}s\")\n",
        "    print(f\"üìù Answer: {response2.choices[0].message.content.strip()[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 2: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TiFeuZyxT4ad",
        "outputId": "d8f3781a-62d9-4a0d-f37a-279a48af5e05"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/64.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0müîë Setting up API Key...\n",
            "‚úÖ Gemini API Key loaded.\n",
            "\n",
            "üìñ Downloading Hamlet from Project Gutenberg...\n",
            "‚úÖ Book loaded! Size: 177967 characters.\n",
            "\n",
            "ü§ñ Starting Experiment with model: gemini/gemini-1.5-flash\n",
            "\n",
            "------------------------------------------------\n",
            "üèÉ RUN 1 (Cold Start)\n",
            "------------------------------------------------\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "‚ùå Error in Run 1: litellm.NotFoundError: GeminiException - {\n",
            "  \"error\": {\n",
            "    \"code\": 404,\n",
            "    \"message\": \"models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\",\n",
            "    \"status\": \"NOT_FOUND\"\n",
            "  }\n",
            "}\n",
            "\n",
            "üí° TIP: If you get a 404, check if 'Generative Language API' is enabled in your Google Cloud Console.\n",
            "\n",
            "------------------------------------------------\n",
            "üèÉ RUN 2 (Simulated Cache)\n",
            "------------------------------------------------\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "‚ùå Error in Run 2: litellm.NotFoundError: GeminiException - {\n",
            "  \"error\": {\n",
            "    \"code\": 404,\n",
            "    \"message\": \"models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\",\n",
            "    \"status\": \"NOT_FOUND\"\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update LiteLLM to the latest version\n",
        "!pip install -Uq litellm requests\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Use the correct model string format for Gemini API v1\n",
        "MODEL = \"gemini-1.5-flash\"  # Changed from \"gemini/gemini-1.5-flash\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: SETUP API KEY\n",
        "# ---------------------------------------------------------\n",
        "print(\"üîë Setting up API Key...\")\n",
        "try:\n",
        "    # Ensure you have added 'GEMINI_API_KEY' in the Colab Secrets (Key icon)\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    if api_key:\n",
        "        os.environ['GEMINI_API_KEY'] = api_key\n",
        "        print(\"‚úÖ Gemini API Key loaded.\")\n",
        "    else:\n",
        "        raise ValueError(\"Key is empty\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Could not find 'GEMINI_API_KEY'. Please check your Colab Secrets.\\n{e}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPARE THE BOOK (FIXED SOURCE)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\nüìñ Downloading Hamlet from Project Gutenberg...\")\n",
        "file_path = \"hamlet.txt\"\n",
        "url = \"https://www.gutenberg.org/cache/epub/1524/pg1524.txt\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    # Save to file\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "    # Read back to verify\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        book_content = f.read()\n",
        "\n",
        "    # REMOVE GUTENBERG HEADER/FOOTER (Optional but cleaner)\n",
        "    if \"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "        book_content = book_content.split(\"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[1]\n",
        "    if \"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "        book_content = book_content.split(\"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[0]\n",
        "\n",
        "    print(f\"‚úÖ Book loaded! Size: {len(book_content)} characters.\")\n",
        "\n",
        "    # ERROR CHECK: If size is small (<1000), the download failed\n",
        "    if len(book_content) < 1000:\n",
        "        raise ValueError(\"The downloaded file is too small. The URL might be blocked.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to download book: {e}\")\n",
        "    book_content = \"To be, or not to be, that is the question.\" * 1000\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: RUN THE EXPERIMENT\n",
        "# ---------------------------------------------------------\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"Here is the book Hamlet:\\n{book_content[:50000]}\\n\\nQuestion: What is the reply to 'Where is my father?'\"}\n",
        "]\n",
        "\n",
        "print(f\"\\nü§ñ Starting Experiment with model: {MODEL}\")\n",
        "\n",
        "# --- RUN 1 ---\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"üèÉ RUN 1 (Cold Start)\")\n",
        "print(\"------------------------------------------------\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response1 = completion(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        api_key=os.environ['GEMINI_API_KEY']\n",
        "    )\n",
        "    elapsed1 = time.time() - start_time\n",
        "    print(f\"‚è±Ô∏è Time: {elapsed1:.2f} seconds\")\n",
        "    print(f\"üìù Answer: {response1.choices[0].message.content.strip()[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 1: {e}\")\n",
        "    print(\"üí° TIP: Make sure the Generative Language API is enabled at:\")\n",
        "    print(\"   https://console.cloud.google.com/apis/library/generativelanguage.googleapis.com\")\n",
        "\n",
        "# --- RUN 2 ---\n",
        "print(\"\\n------------------------------------------------\")\n",
        "print(\"üèÉ RUN 2 (Simulated Cache)\")\n",
        "print(\"------------------------------------------------\")\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response2 = completion(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        api_key=os.environ['GEMINI_API_KEY']\n",
        "    )\n",
        "    elapsed2 = time.time() - start_time\n",
        "    print(f\"‚è±Ô∏è Time: {elapsed2:.2f} seconds\")\n",
        "    if 'elapsed1' in locals():\n",
        "        print(f\"üöÄ Speed Difference: {elapsed1 - elapsed2:.2f}s\")\n",
        "    print(f\"üìù Answer: {response2.choices[0].message.content.strip()[:200]}...\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 2: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*50)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y45RQ_BWZDFb",
        "outputId": "f32ff8f6-bc97-4245-8bcc-99d6fa3ffcbe"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Setting up API Key...\n",
            "‚úÖ Gemini API Key loaded.\n",
            "\n",
            "üìñ Downloading Hamlet from Project Gutenberg...\n",
            "‚úÖ Book loaded! Size: 177967 characters.\n",
            "\n",
            "ü§ñ Starting Experiment with model: gemini-1.5-flash\n",
            "\n",
            "------------------------------------------------\n",
            "üèÉ RUN 1 (Cold Start)\n",
            "------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92m16:15:50 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:550 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 546, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 127, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 268, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 546, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 127, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 268, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "\u001b[92m16:15:50 - LiteLLM:ERROR\u001b[0m: vertex_llm_base.py:550 - Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 546, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 127, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 268, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "ERROR:LiteLLM:Failed to load vertex credentials. Check to see if credentials containing partial/invalid information. Error: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 546, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 127, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 268, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "‚ùå Error in Run 1: litellm.APIConnectionError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/main.py\", line 3262, in completion\n",
            "    model_response = vertex_chat_completion.completion(  # type: ignore\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 2569, in completion\n",
            "    _auth_header, vertex_project = self._ensure_access_token(\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 284, in _ensure_access_token\n",
            "    return self.get_access_token(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 553, in get_access_token\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 546, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 127, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 268, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d954f516ea0>)\n",
            "\n",
            "üí° TIP: Make sure the Generative Language API is enabled at:\n",
            "   https://console.cloud.google.com/apis/library/generativelanguage.googleapis.com\n",
            "\n",
            "------------------------------------------------\n",
            "üèÉ RUN 2 (Simulated Cache)\n",
            "------------------------------------------------\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "‚ùå Error in Run 2: litellm.APIConnectionError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 140, in _refresh_token\n",
            "    self._retrieve_info(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 107, in _retrieve_info\n",
            "    info = _metadata.get_service_account_info(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 342, in get_service_account_info\n",
            "    return get(request, path, params={\"recursive\": \"true\"})\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/_metadata.py\", line 267, in get\n",
            "    raise exceptions.TransportError(\n",
            "google.auth.exceptions.TransportError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/main.py\", line 3262, in completion\n",
            "    model_response = vertex_chat_completion.completion(  # type: ignore\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/gemini/vertex_and_google_ai_studio_gemini.py\", line 2569, in completion\n",
            "    _auth_header, vertex_project = self._ensure_access_token(\n",
            "                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 284, in _ensure_access_token\n",
            "    return self.get_access_token(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 553, in get_access_token\n",
            "    raise e\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 546, in get_access_token\n",
            "    _credentials, credential_project_id = self.load_auth(\n",
            "                                          ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 127, in load_auth\n",
            "    self.refresh_auth(creds)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/litellm/llms/vertex_ai/vertex_llm_base.py\", line 268, in refresh_auth\n",
            "    credentials.refresh(Request())\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/credentials.py\", line 365, in refresh\n",
            "    self._refresh_token(request)\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/google/auth/compute_engine/credentials.py\", line 147, in _refresh_token\n",
            "    raise new_exc from caught_exc\n",
            "google.auth.exceptions.RefreshError: (\"Failed to retrieve http://metadata.google.internal/computeMetadata/v1/instance/service-accounts/default/?recursive=true from the Google Compute Engine metadata service. Status: 404 Response:\\nb''\", <google.auth.transport.requests._Response object at 0x7d9550620830>)\n",
            "\n",
            "\n",
            "==================================================\n",
            "üìä EXPERIMENT COMPLETE\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update LiteLLM to the latest version\n",
        "!pip install -Uq litellm requests\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# CRITICAL: Add \"gemini/\" prefix to force Google AI Studio API (not Vertex AI)\n",
        "MODEL = \"gemini/gemini-1.5-flash-latest\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: SETUP API KEY\n",
        "# ---------------------------------------------------------\n",
        "print(\"üîë Setting up API Key...\")\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    if api_key:\n",
        "        os.environ['GEMINI_API_KEY'] = api_key\n",
        "        print(\"‚úÖ Gemini API Key loaded.\")\n",
        "    else:\n",
        "        raise ValueError(\"Key is empty\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Could not find 'GEMINI_API_KEY'. Please check your Colab Secrets.\\n{e}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPARE THE BOOK (FIXED SOURCE)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\nüìñ Downloading Hamlet from Project Gutenberg...\")\n",
        "file_path = \"hamlet.txt\"\n",
        "url = \"https://www.gutenberg.org/cache/epub/1524/pg1524.txt\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        book_content = f.read()\n",
        "\n",
        "    # Remove Gutenberg header/footer\n",
        "    if \"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "        book_content = book_content.split(\"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[1]\n",
        "    if \"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "        book_content = book_content.split(\"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[0]\n",
        "\n",
        "    print(f\"‚úÖ Book loaded! Size: {len(book_content)} characters.\")\n",
        "\n",
        "    if len(book_content) < 1000:\n",
        "        raise ValueError(\"The downloaded file is too small.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to download book: {e}\")\n",
        "    book_content = \"To be, or not to be, that is the question.\" * 1000\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: RUN THE EXPERIMENT\n",
        "# ---------------------------------------------------------\n",
        "# Note: Using 50k chars to stay within context limits\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"Here is the book Hamlet:\\n\\n{book_content[:50000]}\\n\\nQuestion: What is the reply to 'Where is my father?'\"}\n",
        "]\n",
        "\n",
        "print(f\"\\nü§ñ Starting Experiment with model: {MODEL}\")\n",
        "print(\"üìù Note: Google AI Studio doesn't have explicit prompt caching API yet.\")\n",
        "print(\"   However, Gemini may cache internally for identical prompts.\\n\")\n",
        "\n",
        "# --- RUN 1 ---\n",
        "print(\"=\"*50)\n",
        "print(\"üèÉ RUN 1 (Cold Start)\")\n",
        "print(\"=\"*50)\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response1 = completion(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        # LiteLLM will automatically use GEMINI_API_KEY from environment\n",
        "    )\n",
        "    elapsed1 = time.time() - start_time\n",
        "    answer1 = response1.choices[0].message.content.strip()\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed1:.2f} seconds\")\n",
        "    print(f\"üìù Answer: {answer1[:300]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 1: {e}\")\n",
        "    print(\"\\nüí° TROUBLESHOOTING:\")\n",
        "    print(\"   1. Verify your API key is from Google AI Studio (not Cloud Console)\")\n",
        "    print(\"   2. Get a key from: https://aistudio.google.com/app/apikey\")\n",
        "    print(\"   3. Make sure it's added to Colab Secrets as 'GEMINI_API_KEY'\")\n",
        "\n",
        "# --- RUN 2 ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèÉ RUN 2 (Potential Cache Hit)\")\n",
        "print(\"=\"*50)\n",
        "time.sleep(1)  # Small delay between requests\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response2 = completion(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "    )\n",
        "    elapsed2 = time.time() - start_time\n",
        "    answer2 = response2.choices[0].message.content.strip()\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed2:.2f} seconds\")\n",
        "\n",
        "    if 'elapsed1' in locals():\n",
        "        speedup = elapsed1 - elapsed2\n",
        "        speedup_pct = (speedup / elapsed1) * 100 if elapsed1 > 0 else 0\n",
        "        print(f\"üöÄ Speed Difference: {speedup:.2f}s ({speedup_pct:.1f}% faster)\")\n",
        "\n",
        "        if speedup > 1:\n",
        "            print(\"‚ú® Significant speedup detected! Likely benefiting from internal caching.\")\n",
        "        elif speedup > 0:\n",
        "            print(\"‚ö° Slight speedup - results may vary due to network/server load.\")\n",
        "        else:\n",
        "            print(\"‚è±Ô∏è  Similar speed - caching may not be active for this query.\")\n",
        "\n",
        "    print(f\"üìù Answer: {answer2[:300]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 2: {e}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# SUMMARY\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "if 'elapsed1' in locals() and 'elapsed2' in locals():\n",
        "    print(f\"Run 1: {elapsed1:.2f}s\")\n",
        "    print(f\"Run 2: {elapsed2:.2f}s\")\n",
        "    print(f\"Difference: {elapsed1 - elapsed2:.2f}s\")\n",
        "    print(\"\\nüí° Note: Google AI Studio API doesn't expose explicit caching controls.\")\n",
        "    print(\"   Any speedup is from Gemini's internal optimizations.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XoVqYC2Z8Qe",
        "outputId": "f62814db-dfef-4ab3-acce-a90709564eec"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Setting up API Key...\n",
            "‚úÖ Gemini API Key loaded.\n",
            "\n",
            "üìñ Downloading Hamlet from Project Gutenberg...\n",
            "‚úÖ Book loaded! Size: 177967 characters.\n",
            "\n",
            "ü§ñ Starting Experiment with model: gemini/gemini-1.5-flash-latest\n",
            "üìù Note: Google AI Studio doesn't have explicit prompt caching API yet.\n",
            "   However, Gemini may cache internally for identical prompts.\n",
            "\n",
            "==================================================\n",
            "üèÉ RUN 1 (Cold Start)\n",
            "==================================================\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "‚ùå Error in Run 1: litellm.NotFoundError: GeminiException - {\n",
            "  \"error\": {\n",
            "    \"code\": 404,\n",
            "    \"message\": \"models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\",\n",
            "    \"status\": \"NOT_FOUND\"\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "üí° TROUBLESHOOTING:\n",
            "   1. Verify your API key is from Google AI Studio (not Cloud Console)\n",
            "   2. Get a key from: https://aistudio.google.com/app/apikey\n",
            "   3. Make sure it's added to Colab Secrets as 'GEMINI_API_KEY'\n",
            "\n",
            "==================================================\n",
            "üèÉ RUN 2 (Potential Cache Hit)\n",
            "==================================================\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "\n",
            "\u001b[1;31mProvider List: https://docs.litellm.ai/docs/providers\u001b[0m\n",
            "\n",
            "‚ùå Error in Run 2: litellm.NotFoundError: GeminiException - {\n",
            "  \"error\": {\n",
            "    \"code\": 404,\n",
            "    \"message\": \"models/gemini-1.5-flash-latest is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\",\n",
            "    \"status\": \"NOT_FOUND\"\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "==================================================\n",
            "üìä EXPERIMENT COMPLETE\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Update LiteLLM to the latest version\n",
        "!pip install -Uq litellm requests\n",
        "\n",
        "import os\n",
        "import time\n",
        "import requests\n",
        "from google.colab import userdata\n",
        "from litellm import completion\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Use the standard v1 API model name (not v1beta)\n",
        "MODEL = \"gemini/gemini-1.5-flash\"\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 1: SETUP API KEY\n",
        "# ---------------------------------------------------------\n",
        "print(\"üîë Setting up API Key...\")\n",
        "try:\n",
        "    api_key = userdata.get('GEMINI_API_KEY')\n",
        "    if api_key:\n",
        "        os.environ['GEMINI_API_KEY'] = api_key\n",
        "        print(\"‚úÖ Gemini API Key loaded.\")\n",
        "    else:\n",
        "        raise ValueError(\"Key is empty\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå ERROR: Could not find 'GEMINI_API_KEY'. Please check your Colab Secrets.\\n{e}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 2: PREPARE THE BOOK (FIXED SOURCE)\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\nüìñ Downloading Hamlet from Project Gutenberg...\")\n",
        "file_path = \"hamlet.txt\"\n",
        "url = \"https://www.gutenberg.org/cache/epub/1524/pg1524.txt\"\n",
        "\n",
        "try:\n",
        "    response = requests.get(url)\n",
        "    response.raise_for_status()\n",
        "\n",
        "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(response.text)\n",
        "\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        book_content = f.read()\n",
        "\n",
        "    # Remove Gutenberg header/footer\n",
        "    if \"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "        book_content = book_content.split(\"*** START OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[1]\n",
        "    if \"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\" in book_content:\n",
        "        book_content = book_content.split(\"*** END OF THE PROJECT GUTENBERG EBOOK HAMLET ***\")[0]\n",
        "\n",
        "    print(f\"‚úÖ Book loaded! Size: {len(book_content)} characters.\")\n",
        "\n",
        "    if len(book_content) < 1000:\n",
        "        raise ValueError(\"The downloaded file is too small.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Failed to download book: {e}\")\n",
        "    book_content = \"To be, or not to be, that is the question.\" * 1000\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# STEP 3: RUN THE EXPERIMENT\n",
        "# ---------------------------------------------------------\n",
        "# Use 50k chars to stay within context limits\n",
        "messages = [\n",
        "    {\"role\": \"user\", \"content\": f\"Here is the book Hamlet:\\n\\n{book_content[:50000]}\\n\\nQuestion: What is the reply to 'Where is my father?'\"}\n",
        "]\n",
        "\n",
        "print(f\"\\nü§ñ Starting Experiment with model: {MODEL}\")\n",
        "print(\"üìù Note: Google AI Studio (free tier) doesn't expose explicit caching.\")\n",
        "print(\"   But Gemini may optimize internally for repeated prompts.\\n\")\n",
        "\n",
        "# --- RUN 1 ---\n",
        "print(\"=\"*50)\n",
        "print(\"üèÉ RUN 1 (Cold Start)\")\n",
        "print(\"=\"*50)\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response1 = completion(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        api_base=\"https://generativelanguage.googleapis.com/v1/models\"  # Explicit v1 API\n",
        "    )\n",
        "    elapsed1 = time.time() - start_time\n",
        "    answer1 = response1.choices[0].message.content.strip()\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed1:.2f} seconds\")\n",
        "    print(f\"üìù Answer: {answer1[:300]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 1: {e}\")\n",
        "    print(\"\\nüí° TROUBLESHOOTING:\")\n",
        "    print(\"   ‚Ä¢ Your API key should be from: https://aistudio.google.com/app/apikey\")\n",
        "    print(\"   ‚Ä¢ Make sure 'Generative Language API' is enabled\")\n",
        "    print(\"   ‚Ä¢ Try regenerating your API key if it's old\")\n",
        "\n",
        "    # Try alternative model names\n",
        "    print(\"\\nüîÑ Attempting with alternative model name...\")\n",
        "    try:\n",
        "        response1 = completion(\n",
        "            model=\"gemini/gemini-pro\",\n",
        "            messages=messages\n",
        "        )\n",
        "        elapsed1 = time.time() - start_time\n",
        "        answer1 = response1.choices[0].message.content.strip()\n",
        "        print(f\"‚úÖ Success with gemini-pro!\")\n",
        "        print(f\"‚è±Ô∏è  Time: {elapsed1:.2f} seconds\")\n",
        "        print(f\"üìù Answer: {answer1[:300]}...\")\n",
        "        MODEL = \"gemini/gemini-pro\"  # Update model for Run 2\n",
        "    except Exception as e2:\n",
        "        print(f\"‚ùå Also failed with gemini-pro: {e2}\")\n",
        "\n",
        "# --- RUN 2 ---\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üèÉ RUN 2 (Potential Cache Hit)\")\n",
        "print(\"=\"*50)\n",
        "time.sleep(1)  # Small delay between requests\n",
        "start_time = time.time()\n",
        "try:\n",
        "    response2 = completion(\n",
        "        model=MODEL,\n",
        "        messages=messages,\n",
        "        api_base=\"https://generativelanguage.googleapis.com/v1/models\"\n",
        "    )\n",
        "    elapsed2 = time.time() - start_time\n",
        "    answer2 = response2.choices[0].message.content.strip()\n",
        "\n",
        "    print(f\"‚è±Ô∏è  Time: {elapsed2:.2f} seconds\")\n",
        "\n",
        "    if 'elapsed1' in locals():\n",
        "        speedup = elapsed1 - elapsed2\n",
        "        speedup_pct = (speedup / elapsed1) * 100 if elapsed1 > 0 else 0\n",
        "        print(f\"üöÄ Speed Difference: {speedup:.2f}s ({speedup_pct:.1f}% faster)\")\n",
        "\n",
        "        if speedup > 1:\n",
        "            print(\"‚ú® Significant speedup detected! Likely benefiting from internal caching.\")\n",
        "        elif speedup > 0.2:\n",
        "            print(\"‚ö° Moderate speedup - some optimization may be happening.\")\n",
        "        else:\n",
        "            print(\"‚è±Ô∏è  Similar speed - no significant caching detected.\")\n",
        "\n",
        "    print(f\"üìù Answer: {answer2[:300]}...\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error in Run 2: {e}\")\n",
        "\n",
        "# ---------------------------------------------------------\n",
        "# SUMMARY\n",
        "# ---------------------------------------------------------\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"üìä EXPERIMENT COMPLETE\")\n",
        "print(\"=\"*50)\n",
        "if 'elapsed1' in locals() and 'elapsed2' in locals():\n",
        "    print(f\"Run 1: {elapsed1:.2f}s\")\n",
        "    print(f\"Run 2: {elapsed2:.2f}s\")\n",
        "    print(f\"Difference: {elapsed1 - elapsed2:.2f}s\")\n",
        "    print(\"\\nüí° Notes:\")\n",
        "    print(\"   ‚Ä¢ Free tier doesn't have explicit Context Caching API\")\n",
        "    print(\"   ‚Ä¢ Any speedup is from Gemini's internal optimizations\")\n",
        "    print(\"   ‚Ä¢ For guaranteed caching, you'd need Vertex AI (paid tier)\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Could not complete both runs. Check the errors above.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7mZ6voH4xAR",
        "outputId": "00ceb19a-8fc6-40b4-f1d1-61e395a9fec8"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë Setting up API Key...\n",
            "‚úÖ Gemini API Key loaded.\n",
            "\n",
            "üìñ Downloading Hamlet from Project Gutenberg...\n",
            "‚úÖ Book loaded! Size: 177967 characters.\n",
            "\n",
            "ü§ñ Starting Experiment with model: gemini/gemini-1.5-flash\n",
            "üìù Note: Google AI Studio (free tier) doesn't expose explicit caching.\n",
            "   But Gemini may optimize internally for repeated prompts.\n",
            "\n",
            "==================================================\n",
            "üèÉ RUN 1 (Cold Start)\n",
            "==================================================\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "‚ùå Error in Run 1: litellm.NotFoundError: GeminiException - \n",
            "\n",
            "üí° TROUBLESHOOTING:\n",
            "   ‚Ä¢ Your API key should be from: https://aistudio.google.com/app/apikey\n",
            "   ‚Ä¢ Make sure 'Generative Language API' is enabled\n",
            "   ‚Ä¢ Try regenerating your API key if it's old\n",
            "\n",
            "üîÑ Attempting with alternative model name...\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "‚ùå Also failed with gemini-pro: litellm.NotFoundError: GeminiException - {\n",
            "  \"error\": {\n",
            "    \"code\": 404,\n",
            "    \"message\": \"models/gemini-pro is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\",\n",
            "    \"status\": \"NOT_FOUND\"\n",
            "  }\n",
            "}\n",
            "\n",
            "\n",
            "==================================================\n",
            "üèÉ RUN 2 (Potential Cache Hit)\n",
            "==================================================\n",
            "\n",
            "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
            "LiteLLM.Info: If you need to debug this error, use `litellm._turn_on_debug()'.\n",
            "\n",
            "‚ùå Error in Run 2: litellm.NotFoundError: GeminiException - \n",
            "\n",
            "==================================================\n",
            "üìä EXPERIMENT COMPLETE\n",
            "==================================================\n",
            "‚ö†Ô∏è  Could not complete both runs. Check the errors above.\n"
          ]
        }
      ]
    }
  ]
}