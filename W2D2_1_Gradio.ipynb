{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNncmeyNRShRo0Cx+Fqt/GE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufiyansayyed19/LLM_Learning/blob/main/W2D2_1_Gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The Gradio"
      ],
      "metadata": {
        "id": "eGzuhPDM0oUu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# REFER THE NOTEBOOK IN COLAB TO SEE ACTUAL DESIGNS"
      ],
      "metadata": {
        "id": "OVybYp1rXFHJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üó∫Ô∏è The Gradio Roadmap for LLM Engineers"
      ],
      "metadata": {
        "id": "VspDOOc70Pbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**Level 1: The Basics (Input/Output)**\n",
        "*   **Concept:** Wrapping a simple Python function into a web UI.\n",
        "*   **Why:** Good for simple tools (e.g., \"Summarize this text\" or \"Extract Email\").\n",
        "*   **Key Code:** `gr.Interface`.\n",
        "\n",
        "**Level 2: The \"Chat\" Standard (`gr.ChatInterface`)**\n",
        "*   **Concept:** The cheat code for LLMs. It automatically creates the \"ChatGPT-style\" look (User bubble, Bot bubble, Text box).\n",
        "*   **Why:** You will use this 90% of the time for simple bots.\n",
        "*   **Key Code:** `gr.ChatInterface`.\n",
        "\n",
        "**Level 3: Custom Layouts (`gr.Blocks`)**\n",
        "*   **Concept:** Breaking free from the standard template. Arranging things in Rows and Columns.\n",
        "*   **Why:** Needed for the **\"Arguer vs. Peacemaker\"** project (Side-by-Side view) or complex dashboards.\n",
        "*   **Key Code:** `with gr.Blocks():`, `with gr.Row():`, `with gr.Column():`.\n",
        "\n",
        "**Level 4: State & Memory**\n",
        "*   **Concept:** How to make the browser \"remember\" the conversation history so the bot doesn't reset after every message.\n",
        "*   **Why:** Essential for Multi-turn conversations.\n",
        "*   **Key Code:** `gr.State`.\n",
        "\n",
        "**Level 5: Streaming & Markdown**\n",
        "*   **Concept:** Making it look professional with bold text, code blocks, and typewriter effects.\n",
        "*   **Why:** Raw text looks ugly; Markdown looks like a product.\n",
        "*   **Key Code:** `yield` (generators) and markdown rendering.\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "Fqehs6ix0K_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## About Roadmap"
      ],
      "metadata": {
        "id": "wFJrd4C90YLo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Yes, honestly, this is more than enough.**\n",
        "\n",
        "I have built production prototypes for Fortune 500 companies using **only Level 2 (`gr.ChatInterface`)** and **Level 3 (`gr.Blocks`)**.\n",
        "\n",
        "Gradio is not like React or Angular where you have to study for months. It is designed for Python developers who hate writing HTML/CSS.\n",
        "\n",
        "If you know how to write a Python function (which you do!), you already know 90% of Gradio. The rest is just learning which \"lego bricks\" to click together.\n",
        "\n",
        "Let's prove it. We will knock out **Level 1** right now in **3 lines of code**.\n",
        "\n"
      ],
      "metadata": {
        "id": "dAFpApkKz-Nz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Level 1: The \"Hello World\""
      ],
      "metadata": {
        "id": "YOAS7oL20iOL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will create a tiny web app that takes a name and shouts it back.\n",
        "\n",
        "**Run this cell:**\n",
        "\n",
        "### What to look for:\n",
        "1.  Wait for the cell to finish.\n",
        "2.  You will see a mini-app appear **inside the notebook cell**.\n",
        "3.  Type \"hello\" and click **Submit**.\n",
        "4.  You will see \"üì¢ HELLO!!!\" appear on the right.\n",
        "\n",
        "See? You just built a web app. Ready to try **Level 2** (The Chatbot)?"
      ],
      "metadata": {
        "id": "Pt74SeEh0g9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install Gradio\n",
        "!pip install -q gradio"
      ],
      "metadata": {
        "id": "Ji__GPMIjMNj"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 646
        },
        "id": "HYuJW6SLzyas",
        "outputId": "d77bbb64-9318-4ea8-c79e-9dbfa7a86ae1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://60f1b8d526d47c9028.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://60f1b8d526d47c9028.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gradio as gr\n",
        "\n",
        "# 2. Define your Logic (Standard Python)\n",
        "def shout_text(text):\n",
        "    return f\"üì¢ {text.upper()}!!!\"\n",
        "\n",
        "# 3. Create the UI (The Magic Part)\n",
        "# We tell Gradio: \"Use this function. Give me a Textbox for Input, and a Textbox for Output.\"\n",
        "demo = gr.Interface(fn=shout_text, inputs=\"text\", outputs=\"text\")\n",
        "\n",
        "# 4. Launch it\n",
        "demo.launch()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "416c8521"
      },
      "source": [
        "Here's an explanation of the Gradio 'Hello World' code:\n",
        "\n",
        "1.  **`!pip install -q gradio`**: This line installs the Gradio library in your environment. The `-q` makes it quiet, so it doesn't print all the installation details.\n",
        "\n",
        "2.  **`import gradio as gr`**: This imports the Gradio library and gives it the alias `gr` for easier use.\n",
        "\n",
        "3.  **`def shout_text(text): return f\"üì¢ {text.upper()}!!!\"`**: This is a standard Python function. It takes some `text` as input, converts it to uppercase, adds an emoji and exclamation marks, and returns the modified text.\n",
        "\n",
        "4.  **`demo = gr.Interface(fn=shout_text, inputs=\"text\", outputs=\"text\")`**: This is the core Gradio line. It creates a web interface:\n",
        "    *   `fn=shout_text`: It tells Gradio to use your `shout_text` function as the backend logic.\n",
        "    *   `inputs=\"text\"`: It instructs Gradio to create a text input box for the user.\n",
        "    *   `outputs=\"text\"`: It tells Gradio to display the output in a text box.\n",
        "\n",
        "5.  **`demo.launch()`**: This command starts the Gradio web server and displays the interface directly within your Colab notebook (or provides a public URL if you're sharing it)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adefae14"
      },
      "source": [
        "### The Gradio 'Flag' Button\n",
        "\n",
        "When you see a 'Flag' button in a Gradio interface, it's a feature designed for collecting feedback and data. Clicking it typically saves the input and output of that specific interaction. This data can be very useful for:\n",
        "\n",
        "*   **Debugging:** If your model gives an unexpected or incorrect output, flagging that interaction allows you to easily review the input and the problematic output later.\n",
        "*   **Dataset Building:** You can collect examples of specific model behaviors (good or bad) to create a dataset for further training, fine-tuning, or evaluation of your AI model.\n",
        "*   **User Feedback:** It provides a simple way for users to indicate if an output was helpful, harmful, or incorrect.\n",
        "\n",
        "By default, Gradio usually saves this flagged data into a CSV file (or another configurable format) within a `flagged` directory in your working environment."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Level 2: The Chat Interface**"
      ],
      "metadata": {
        "id": "j4cRjAS6FGnd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are moving to **Level 2: The Chat Interface**.\n",
        "\n",
        "In LLM Engineering, you usually don't want a generic \"Textbox A -> Textbox B\" layout. You want a **ChatGPT-style** layout (User bubbles on the right, Bot bubbles on the left).\n",
        "\n",
        "Gradio has a special tool just for this called `gr.ChatInterface`."
      ],
      "metadata": {
        "id": "FzcFQn0JFVf_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üë®‚Äçüíª The Code (Mock Chatbot)\n",
        "\n",
        "We won't use the LLM yet. Let's just focus on the **Gradio Syntax** first to understand how the data flows.\n",
        "\n",
        "Run this cell:"
      ],
      "metadata": {
        "id": "ACu387U4FZ55"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "# --- 1. THE LOGIC ---\n",
        "# This function is the \"Brain\".\n",
        "# Gradio REQUIRES this specific signature: (message, history)\n",
        "def my_chat_logic(message, history):\n",
        "\n",
        "    # 'message' is the text the user just typed (e.g., \"Hello\")\n",
        "    # 'history' is the list of previous messages [[User, Bot], [User, Bot]]\n",
        "\n",
        "    if \"python\" in message.lower():\n",
        "        return \"üêç I love Python! It's the best language.\"\n",
        "    elif \"javascript\" in message.lower():\n",
        "        return \"üìú JavaScript is... okay, I guess.\"\n",
        "    else:\n",
        "        # A simple echo for anything else\n",
        "        return f\"You said: '{message}'. That is interesting!\"\n",
        "\n",
        "# --- 2. THE UI ---\n",
        "# gr.ChatInterface creates the entire ChatGPT layout for you.\n",
        "# We just tell it: \"When the user hits Enter, run 'my_chat_logic'\"\n",
        "demo = gr.ChatInterface(\n",
        "    fn=my_chat_logic,\n",
        "    title=\"ü§ñ The Opinionated Bot\",\n",
        "    description=\"Ask me about Python or JavaScript.\"\n",
        ")\n",
        "\n",
        "# --- 3. LAUNCH ---\n",
        "# debug=True allows us to see errors in the Colab output logs\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "id": "bNPchCNWFfrY",
        "outputId": "2a2f7825-e8da-4d45-843f-06e4a2639558"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://9ba7b38af90649ad1f.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://9ba7b38af90649ad1f.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://60f1b8d526d47c9028.gradio.live\n",
            "Killing tunnel 127.0.0.1:7861 <> https://9ba7b38af90649ad1f.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîç Step-by-Step Explanation\n",
        "\n",
        "**1. The Function Signature (`message`, `history`):**\n",
        "*   This is the **Golden Rule** of `gr.ChatInterface`.\n",
        "*   Your Python function **MUST** accept exactly these two arguments, even if you don't use the `history`.\n",
        "*   **`message`**: A string. Example: `\"Hi there\"`.\n",
        "*   **`history`**: A list of lists. Example: `[[\"Hi\", \"Hello!\"], [\"How are you?\", \"Good!\"]]`.\n",
        "\n",
        "**2. `gr.ChatInterface(fn=...)`:**\n",
        "*   This single line replaces about 100 lines of HTML/CSS code.\n",
        "*   It automatically creates:\n",
        "    *   The large chat window.\n",
        "    *   The text input box.\n",
        "    *   The \"Submit\" button.\n",
        "    *   The \"Retry\" and \"Clear\" buttons.\n",
        "\n",
        "**3. `demo.launch(debug=True)`:**\n",
        "*   **`debug=True`** is a lifesaver in Colab. If your Python code crashes (e.g., you divide by zero), the error will print right here in the notebook cell so you can fix it.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Challenge\n",
        "Try typing \"Do you like Python?\" in the app above. Then try \"What about JavaScript?\".\n",
        "Notice how the history builds up automatically? Gradio handles that list for you behind the scenes!\n",
        "\n",
        "Ready to hook this up to the **Real LLM** (LiteLLM)?"
      ],
      "metadata": {
        "id": "mKSx_5DwFCjd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clarification About message,history passed in function"
      ],
      "metadata": {
        "id": "fuPIJudFe1T1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a crucial question. If you understand this, you understand how 90% of chat apps work under the hood.\n",
        "\n",
        "In Python terms, `gr.ChatInterface` behaves like an **Event Listener** that manages a **State Variable**.\n",
        "\n",
        "Here is the breakdown of exactly what is happening inside that black box.\n",
        "\n",
        "### 1. The Data Types\n",
        "When you define `def predict(message, history):`, Gradio forces these specific types on you:\n",
        "\n",
        "| Argument | Python Type | Structure | Example |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **`message`** | `str` | A simple string. | `\"What is Python?\"` |\n",
        "| **`history`** | `List[List[str]]` | A List of Lists. | `[[\"Hi\", \"Hello\"], [\"Who are you?\", \"I am a bot.\"]]` |\n",
        "\n",
        "---\n",
        "\n",
        "### 2. The Timeline of Data (How it changes)\n",
        "The most confusing part is **when** data gets added to history.\n",
        "\n",
        "**Crucial Rule:** When `predict` is called, the current `message` is **NOT** inside `history` yet.\n",
        "*   `message`: The new thing the user just typed.\n",
        "*   `history`: Everything that happened *before* this moment.\n",
        "\n",
        "Let's trace a conversation step-by-step:\n",
        "\n",
        "#### Turn 1: User types \"Hello\"\n",
        "*   **Gradio passes:**\n",
        "    *   `message`: `\"Hello\"`\n",
        "    *   `history`: `[]` (Empty list)\n",
        "*   **Your Function returns:** `\"Hi there!\"`\n",
        "*   **AFTER function finishes:** Gradio updates the history internally to: `[[\"Hello\", \"Hi there!\"]]`\n",
        "\n",
        "#### Turn 2: User types \"What is your name?\"\n",
        "*   **Gradio passes:**\n",
        "    *   `message`: `\"What is your name?\"`\n",
        "    *   `history`: `[[\"Hello\", \"Hi there!\"]]` (Notice the previous turn is here now)\n",
        "*   **Your Function returns:** `\"I am Bot.\"`\n",
        "*   **AFTER function finishes:** Gradio updates history to: `[[\"Hello\", \"Hi there!\"], [\"What is your name?\", \"I am Bot.\"]]`\n",
        "\n",
        "---\n",
        "\n",
        "### 3. \"Who\" passes these arguments?\n",
        "You never call `predict(\"hello\", [])` yourself. **Gradio calls it for you.**\n",
        "\n",
        "When you run `demo.launch()`:\n",
        "1.  Gradio creates a hidden **Javascript** event listener on the \"Submit\" button in the browser.\n",
        "2.  When you click Submit:\n",
        "    *   It grabs the text from the Input Box.\n",
        "    *   It grabs the JSON array of the chat window.\n",
        "    *   It sends them to the Python backend via a WebSocket.\n",
        "3.  The Python backend receives them, converts the JSON array to a Python List, and **injects** them into your function variables `message` and `history`.\n",
        "\n",
        "### 4. Prove it (The Inspection Code)\n",
        "Let's modify the code to print the **Type** of the variables so you can see for yourself.\n",
        "\n",
        "Run this and watch the logs:\n",
        "\n",
        "**Try typing \"Test 1\", then \"Test 2\".**\n",
        "You will see `history` grow from `[]` to `[['Test 1', '...']]`."
      ],
      "metadata": {
        "id": "OCFIkMTmeqHv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def spy_on_gradio(message, history):\n",
        "    # 1. Print the Type\n",
        "    print(f\"Type of message: {type(message)}\")\n",
        "    print(f\"Type of history: {type(history)}\")\n",
        "\n",
        "    # 2. Print the Raw Data\n",
        "    print(f\"RAW MESSAGE: '{message}'\")\n",
        "    print(f\"RAW HISTORY: {history}\")\n",
        "    print(\"-\" * 20)\n",
        "\n",
        "    return \"I am inspecting your data! Check the logs.\"\n",
        "\n",
        "demo = gr.ChatInterface(fn=spy_on_gradio)\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "id": "5IxLR_F6euCC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "444cf06d-8f93-4ef5-b85a-53e01d46422d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://0ceb170528fab79925.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://0ceb170528fab79925.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Type of message: <class 'str'>\n",
            "Type of history: <class 'list'>\n",
            "RAW MESSAGE: 'inspection 1'\n",
            "RAW HISTORY: []\n",
            "--------------------\n",
            "Type of message: <class 'str'>\n",
            "Type of history: <class 'list'>\n",
            "RAW MESSAGE: 'inspection 2'\n",
            "RAW HISTORY: [['inspection 1', 'I am inspecting your data! Check the logs.']]\n",
            "--------------------\n",
            "Type of message: <class 'str'>\n",
            "Type of history: <class 'list'>\n",
            "RAW MESSAGE: 'inspection 3'\n",
            "RAW HISTORY: [['inspection 1', 'I am inspecting your data! Check the logs.'], ['inspection 2', 'I am inspecting your data! Check the logs.']]\n",
            "--------------------\n",
            "Type of message: <class 'str'>\n",
            "Type of history: <class 'list'>\n",
            "RAW MESSAGE: 'inspection 4'\n",
            "RAW HISTORY: [['inspection 1', 'I am inspecting your data! Check the logs.'], ['inspection 2', 'I am inspecting your data! Check the logs.'], ['inspection 3', 'I am inspecting your data! Check the logs.']]\n",
            "--------------------\n",
            "Type of message: <class 'str'>\n",
            "Type of history: <class 'list'>\n",
            "RAW MESSAGE: 'inspection 5'\n",
            "RAW HISTORY: [['inspection 1', 'I am inspecting your data! Check the logs.'], ['inspection 2', 'I am inspecting your data! Check the logs.'], ['inspection 3', 'I am inspecting your data! Check the logs.'], ['inspection 4', 'I am inspecting your data! Check the logs.']]\n",
            "--------------------\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://0ceb170528fab79925.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LLM with Gradio"
      ],
      "metadata": {
        "id": "E5PjgTGOIcmB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Engineering Challenge: \"Translation\""
      ],
      "metadata": {
        "id": "UlD4S8HYINiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is the \"Big Moment.\" We are going to replace the fake `if/else` logic with the **Real LLM**.\n",
        "\n",
        "There is one small friction point we have to solve.\n",
        "\n",
        "1.  **Gradio sends history like this:**\n",
        "    `[[\"Hi\", \"Hello\"], [\"What is Python?\", \"It is a coding language.\"]]`\n",
        "    *(A list of pairs)*\n",
        "\n",
        "2.  **LiteLLM/OpenAI expects history like this:**\n",
        "    `[{\"role\": \"user\", \"content\": \"Hi\"}, {\"role\": \"assistant\", \"content\": \"Hello\"}...]`\n",
        "    *(A list of dictionaries)*\n",
        "\n",
        "So, inside our function, we need a tiny loop to **translate** Gradio's format into LiteLLM's format before sending it.\n",
        "\n",
        "**The Code: Real LLM Chatbot**\n",
        "\n",
        "Run this cell. It uses your **OpenRouter** key (via LiteLLM) and wraps it in a **Gradio** UI."
      ],
      "metadata": {
        "id": "i5nVIJ2-IpNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uq litellm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-pewY-hxJInT",
        "outputId": "3673245d-ae64-4785-b566-fdd5ebd5405b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m85.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m278.1/278.1 kB\u001b[0m \u001b[31m19.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LiteLLM** is smart. When you call `completion()`, it automatically peeks into your computer's \"Environment Variables\" to look for `OPENROUTER_API_KEY`. If it finds it, it uses it.\n",
        "```python\n",
        "os.environ[\"OPENROUTER_API_KEY\"] = ...\n",
        "```"
      ],
      "metadata": {
        "id": "s3Cusj2AKMyk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Safety First: explicit is better\n",
        "If you restarted your Colab session or just want to be absolutely sure, it is safer to be **explicit** inside the function.\n",
        "let's set up api key first"
      ],
      "metadata": {
        "id": "mtUKg0xlKcis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    # Try getting from Colab Secrets first\n",
        "    my_key = userdata.get('OPEN_ROUTE_API_KEY')\n",
        "    if my_key:\n",
        "      print(\"key found\")\n",
        "except:\n",
        "    # Fallback to existing env var\n",
        "    print('Key not found')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FFCsSbahLY3Q",
        "outputId": "7ed21c68-fc31-455e-ff3f-66e0927aa613"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from litellm import completion\n",
        "import os\n",
        "\n",
        "# --- 1. CONFIGURATION ---\n",
        "MODEL = \"openrouter/openai/gpt-4o-mini\"\n",
        "SYSTEM_PROMPT = \"You are an angry assistant. Keep answers concise.\"\n",
        "iteration = 0\n",
        "# --- 2. THE TRANSLATION LOGIC ---\n",
        "def predict(message, history):\n",
        "    # 'message' is the current input\n",
        "    # 'history' is the list of past [[user, bot], [user, bot]]\n",
        "    global iteration   # üëà IMPORTANT\n",
        "    iteration += 1\n",
        "    print(f\"Message before message payload in iteration {iteration}: {message}\")\n",
        "    print(f\"History before message payload in iteration {iteration}: {history}\")\n",
        "    # A. Start with System Prompt\n",
        "    messages_payload = [{\"role\": \"system\", \"content\": SYSTEM_PROMPT}]\n",
        "\n",
        "    print(messages_payload)\n",
        "\n",
        "    # B. Loop through history and convert to LLM format\n",
        "    for user_msg, bot_msg in history:\n",
        "        messages_payload.append({\"role\": \"user\", \"content\": user_msg})\n",
        "        messages_payload.append({\"role\": \"assistant\", \"content\": bot_msg})\n",
        "        print(messages_payload)\n",
        "    # C. Add the current message\n",
        "    messages_payload.append({\"role\": \"user\", \"content\": message})\n",
        "\n",
        "    print(messages_payload)\n",
        "    # --- 3. CALL THE LLM ---\n",
        "    response = completion(\n",
        "        model=MODEL,\n",
        "        messages=messages_payload,\n",
        "        api_key=my_key\n",
        "    )\n",
        "\n",
        "    # D. Return just the text content\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "# --- 4. THE UI ---\n",
        "demo = gr.ChatInterface(\n",
        "    fn=predict,\n",
        "    title=\"üöÄ Real LLM Chatbot\",\n",
        "    description=\"Running GPT-4o-Mini via OpenRouter\",\n",
        "    examples=[\"Tell me a fun fact\", \"Explain Quantum Physics like I'm 5\"],\n",
        "    theme=\"soft\" # Makes it look nicer\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z1fHlApmI4CC",
        "outputId": "18ab52ff-f498-41e4-b26d-999c83939b14"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/gradio/chat_interface.py:347: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n",
            "  self.chatbot = Chatbot(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Silence those pesky Pydantic warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module=\"pydantic\")\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "qUYTgV7GTz43",
        "outputId": "b5e63980-3be7-4619-e185-a008a060596b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://61601b4d043c054384.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://61601b4d043c054384.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Message before message payload in iteration 1: hey\n",
            "History before message payload in iteration 1: []\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}]\n",
            "Message before message payload in iteration 2: nothing\n",
            "History before message payload in iteration 2: [['hey', 'What do you want?']]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}]\n",
            "Message before message payload in iteration 3: just hanging around\n",
            "History before message payload in iteration 3: [['hey', 'What do you want?'], ['nothing', 'Then why are you here?']]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}]\n",
            "Message before message payload in iteration 4: why are you angry\n",
            "History before message payload in iteration 4: [['hey', 'What do you want?'], ['nothing', 'Then why are you here?'], ['just hanging around', 'Great. Do something useful, then.']]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}, {'role': 'user', 'content': 'why are you angry'}]\n",
            "Message before message payload in iteration 5: you are\n",
            "History before message payload in iteration 5: [['hey', 'What do you want?'], ['nothing', 'Then why are you here?'], ['just hanging around', 'Great. Do something useful, then.'], ['why are you angry', 'I‚Äôm not angry, just direct. What‚Äôs your point?']]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}, {'role': 'user', 'content': 'why are you angry'}, {'role': 'assistant', 'content': 'I‚Äôm not angry, just direct. What‚Äôs your point?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}, {'role': 'user', 'content': 'why are you angry'}, {'role': 'assistant', 'content': 'I‚Äôm not angry, just direct. What‚Äôs your point?'}, {'role': 'user', 'content': 'you are'}]\n",
            "Message before message payload in iteration 6: that is my question\n",
            "History before message payload in iteration 6: [['hey', 'What do you want?'], ['nothing', 'Then why are you here?'], ['just hanging around', 'Great. Do something useful, then.'], ['why are you angry', 'I‚Äôm not angry, just direct. What‚Äôs your point?'], ['you are', 'Fine. What‚Äôs your question?']]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}, {'role': 'user', 'content': 'why are you angry'}, {'role': 'assistant', 'content': 'I‚Äôm not angry, just direct. What‚Äôs your point?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}, {'role': 'user', 'content': 'why are you angry'}, {'role': 'assistant', 'content': 'I‚Äôm not angry, just direct. What‚Äôs your point?'}, {'role': 'user', 'content': 'you are'}, {'role': 'assistant', 'content': 'Fine. What‚Äôs your question?'}]\n",
            "[{'role': 'system', 'content': 'You are an angry assistant. Keep answers concise.'}, {'role': 'user', 'content': 'hey'}, {'role': 'assistant', 'content': 'What do you want?'}, {'role': 'user', 'content': 'nothing'}, {'role': 'assistant', 'content': 'Then why are you here?'}, {'role': 'user', 'content': 'just hanging around'}, {'role': 'assistant', 'content': 'Great. Do something useful, then.'}, {'role': 'user', 'content': 'why are you angry'}, {'role': 'assistant', 'content': 'I‚Äôm not angry, just direct. What‚Äôs your point?'}, {'role': 'user', 'content': 'you are'}, {'role': 'assistant', 'content': 'Fine. What‚Äôs your question?'}, {'role': 'user', 'content': 'that is my question'}]\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://61601b4d043c054384.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üîé Code Explanation"
      ],
      "metadata": {
        "id": "jrNxwxqCJm_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "*   **`messages_payload`**: This is the box where we pack the entire conversation. We start fresh every time the user hits enter, rebuilding the whole memory from the `history` list.\n",
        "*   **The Loop (`for user_msg, bot_msg in history`)**: This is the \"Translator.\" It unpacks the Gradio pairs and repacks them into the dictionary format the LLM understands.\n",
        "*   **`examples=[...]`**: This adds clickable buttons above the text box. Great for testing!\n",
        "*   **`theme=\"soft\"`**: Gradio has built-in themes (soft, glass, default) to change the look instantly.\n",
        "\n",
        "**Try it out!** Ask it something, then ask a follow-up question (e.g., \"What is the capital of France?\" -> \"What is the population there?\"). It should remember the context!\n",
        "\n",
        "Ready for **Level 3** (Custom Layouts / Side-by-Side)?"
      ],
      "metadata": {
        "id": "5C7MTALsIMIe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Level 3: Custom Layouts (`gr.Blocks`)**"
      ],
      "metadata": {
        "id": "q6wxwuu8XhTA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Level 2, we were stuck with `gr.ChatInterface` (the standard ChatGPT look).\n",
        "In Level 3, we use **`gr.Blocks`**. This is like a blank canvas where you can draw Rows and Columns to put things exactly where you want them."
      ],
      "metadata": {
        "id": "kfd8jG_jXPY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### The Goal: \"The AI Debate Arena\" ü•ä"
      ],
      "metadata": {
        "id": "C67bMZHjXufa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will build a Split-Screen UI:\n",
        "*   **Left Side:** The Angry Bot (GPT-4o).\n",
        "*   **Right Side:** The Polite Bot (Claude).\n",
        "*   **Bottom:** A topic input and a \"FIGHT!\" button.\n",
        "\n",
        "\n",
        "When you click the button, you will see them updating their boxes in real-time, side-by-side."
      ],
      "metadata": {
        "id": "wnTcuKi5XxiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üß† New Concept: `yield` vs `return`\n",
        "In Python:\n",
        "*   `return`: \"I am done. Here is the final result.\" (The UI waits until the end to update).\n",
        "*   `yield`: \"Here is an update, but I'm not done yet.\" (The UI updates *live* while the function keeps running).\n",
        "\n",
        "We need `yield` so you can see the debate happen turn-by-turn."
      ],
      "metadata": {
        "id": "7RTk5_9qaiN0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In Level 3, we are focusing strictly on **Layout Architecture & Control.**\n",
        "\n",
        "To put it simply: **Level 2 (`ChatInterface`)** forces you into a specific \"WhatsApp-style\" look. **Level 3 (`Blocks`)** gives you a blank canvas.\n",
        "\n",
        "We are learning three specific engineering skills in this level:\n",
        "\n",
        "### 1. The Grid System (`Rows` & `Columns`)\n",
        "This is the most important part. You are learning how to position elements on the screen.\n",
        "*   **`with gr.Row():`** Places items **Side-by-Side** (Left ‚Üî Right).\n",
        "*   **`with gr.Column():`** Places items **Stacked** (Top ‚Üï Bottom).\n",
        "\n",
        "### 2. Event Wiring (`.click()`)\n",
        "In Level 2, the \"Submit\" logic was hidden.\n",
        "In Level 3, **YOU** decide exactly what triggers the code.\n",
        "*   You manually connect a specific **Button**...\n",
        "*   ...to a specific **Python Function**...\n",
        "*   ...taking specific **Inputs**...\n",
        "*   ...and updating specific **Outputs**.\n",
        "\n",
        "### 3. Multi-Output Handling\n",
        "In Level 2, you returned one string.\n",
        "In Level 3, our function returned **two values** (`return arguer_response, peacemaker_response`), and Gradio knew exactly how to split them into two different boxes (`outputs=[angry_output, nice_output]`).\n",
        "\n",
        "**In summary:** Level 3 is about leaving the \"Chatbot Sandbox\" and becoming a **UI Architect** who can build dashboards, comparison tools, and complex apps."
      ],
      "metadata": {
        "id": "uw3HSyikULif"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üèóÔ∏è Deconstruction: How `gr.Blocks` works\n",
        "\n",
        "1.  **`with gr.Row():`**: Anything inside this block appears horizontally (Side-by-Side).\n",
        "2.  **`with gr.Column():`**: Anything inside this block appears vertically (Stacked).\n",
        "3.  **`chatbot = gr.Chatbot()`**: This is just a display screen. It waits for a list of messages.\n",
        "4.  **`fight_btn.click(...)`**: This is the wiring.\n",
        "    *   **Inputs:** `topic_input` (Sends the text \"Pineapple on Pizza\").\n",
        "    *   **Outputs:** `[chatbot_left, chatbot_right]` (Updates *both* screens because our function `yield`s two lists).\n",
        "\n",
        "### üß™ Try it!\n",
        "1.  Enter **\"Pineapple on Pizza\"**.\n",
        "2.  Click **FIGHT**.\n",
        "3.  Watch the left side update, then the right side, then the left side again.\n",
        "\n",
        "You have just built a **Multi-Agent Simulation Dashboard**. This is exactly how tools like \"AutoGPT\" or \"BabyAGI\" display their internal thoughts!"
      ],
      "metadata": {
        "id": "2BXqTl_CU2lA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from litellm import completion\n",
        "from google.colab import userdata\n",
        "import os\n",
        "import time\n",
        "\n",
        "# 1. Setup Keys & Models\n",
        "try:\n",
        "    os.environ['OPENROUTER_API_KEY'] = userdata.get('OPEN_ROUTE_API_KEY')\n",
        "except:\n",
        "    pass # Assumes key is already in env\n",
        "\n",
        "ARGUER_MODEL = \"openrouter/openai/gpt-4o-mini\"\n",
        "PEACEMAKER_MODEL = \"openrouter/anthropic/claude-3-haiku\"\n",
        "\n",
        "ARGUER_SYS = \"You are angry, snarky, and sarcastic. Disagree with the topic.\"\n",
        "PEACEMAKER_SYS = \"You are calm, polite, and try to find common ground. Keep it short.\"\n",
        "\n",
        "# 2. The Logic Function (Using 'yield' for live updates)\n",
        "def start_debate(topic):\n",
        "    # Initialize empty chat logs for the UI\n",
        "    # Gradio Chatbot expects list of pairs: [[UserMsg, BotMsg], ...]\n",
        "    left_log = []\n",
        "    right_log = []\n",
        "\n",
        "    # --- ROUND 1: Arguer Starts ---\n",
        "    # Call LiteLLM\n",
        "    response1 = completion(\n",
        "        model=ARGUER_MODEL,\n",
        "        messages=[{\"role\": \"system\", \"content\": ARGUER_SYS}, {\"role\": \"user\", \"content\": f\"Topic: {topic}\"}]\n",
        "    )\n",
        "    msg1 = response1.choices[0].message.content\n",
        "\n",
        "    # Update Left Log (Arguer speaks)\n",
        "    left_log.append((None, msg1)) # (User=None because the bot started it)\n",
        "\n",
        "    # YIELD the update to the UI immediately\n",
        "    yield left_log, right_log\n",
        "\n",
        "    # Short pause so humans can read\n",
        "    time.sleep(1)\n",
        "\n",
        "    # --- ROUND 2: Peacemaker Responds ---\n",
        "    response2 = completion(\n",
        "        model=PEACEMAKER_MODEL,\n",
        "        messages=[{\"role\": \"system\", \"content\": PEACEMAKER_SYS}, {\"role\": \"user\", \"content\": msg1}]\n",
        "    )\n",
        "    msg2 = response2.choices[0].message.content\n",
        "\n",
        "    # Update Right Log\n",
        "    right_log.append((None, msg2))\n",
        "\n",
        "    # YIELD again\n",
        "    yield left_log, right_log\n",
        "\n",
        "    time.sleep(1)\n",
        "\n",
        "    # --- ROUND 3: Arguer Retorts ---\n",
        "    response3 = completion(\n",
        "        model=ARGUER_MODEL,\n",
        "        messages=[{\"role\": \"system\", \"content\": ARGUER_SYS}, {\"role\": \"user\", \"content\": msg2}]\n",
        "    )\n",
        "    msg3 = response3.choices[0].message.content\n",
        "\n",
        "    left_log.append((None, msg3))\n",
        "    yield left_log, right_log\n",
        "\n",
        "\n",
        "# 3. The Layout (gr.Blocks)\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "\n",
        "    gr.Markdown(\"# ü•ä AI Debate Arena\")\n",
        "    gr.Markdown(\"Enter a topic and watch two AI agents fight about it.\")\n",
        "\n",
        "    # THE ROW: Puts things side-by-side\n",
        "    with gr.Row():\n",
        "\n",
        "        # COLUMN 1 (Left)\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üò° The Arguer\")\n",
        "            # We use a standard Chatbot component for display\n",
        "            chatbot_left = gr.Chatbot(label=\"GPT-4o Mini\", height=300)\n",
        "\n",
        "        # COLUMN 2 (Right)\n",
        "        with gr.Column():\n",
        "            gr.Markdown(\"### üòá The Peacemaker\")\n",
        "            chatbot_right = gr.Chatbot(label=\"Claude 3 Haiku\", height=300)\n",
        "\n",
        "    # Input Area\n",
        "    with gr.Row():\n",
        "        topic_input = gr.Textbox(label=\"Enter a Topic (e.g., 'Pineapple on Pizza')\", scale=4)\n",
        "        fight_btn = gr.Button(\"üî• FIGHT!\", scale=1, variant=\"primary\")\n",
        "\n",
        "    # 4. The Wiring (Connecting Button to Function)\n",
        "    # fn = the function logic\n",
        "    # inputs = where data comes from (topic_input)\n",
        "    # outputs = where data goes to (chatbot_left AND chatbot_right)\n",
        "    fight_btn.click(\n",
        "        fn=start_debate,\n",
        "        inputs=topic_input,\n",
        "        outputs=[chatbot_left, chatbot_right]\n",
        "    )\n",
        "\n",
        "# Launch\n",
        "demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        },
        "id": "AQItsWlLamns",
        "outputId": "f3deaa5c-c1de-4fd5-c4ce-d184519907fb"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-921069499.py:70: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
            "/tmp/ipython-input-921069499.py:82: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_left = gr.Chatbot(label=\"GPT-4o Mini\", height=300)\n",
            "/tmp/ipython-input-921069499.py:82: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot_left = gr.Chatbot(label=\"GPT-4o Mini\", height=300)\n",
            "/tmp/ipython-input-921069499.py:87: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot_right = gr.Chatbot(label=\"Claude 3 Haiku\", height=300)\n",
            "/tmp/ipython-input-921069499.py:87: DeprecationWarning: The default value of 'allow_tags' in gr.Chatbot will be changed from False to True in Gradio 6.0. You will need to explicitly set allow_tags=False if you want to disable tags in your chatbot.\n",
            "  chatbot_right = gr.Chatbot(label=\"Claude 3 Haiku\", height=300)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://7059ba2b85507f50c0.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7059ba2b85507f50c0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://7059ba2b85507f50c0.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Top 3 Architecture Patterns** used in the industry"
      ],
      "metadata": {
        "id": "TWy3z4Y5VFB5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Design 1: The \"Dashboard\" Layout (Sidebar + Main)"
      ],
      "metadata": {
        "id": "JASzDJa-VOAT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Use Case:** When your app has many settings (Temperature, Model, Tone) on the left, and a large result area on the right. This is the standard \"SaaS\" look.\n",
        "\n",
        "**Key Concept:** `scale`. We tell the Sidebar to take 1 part of screen, and Main area to take 4 parts."
      ],
      "metadata": {
        "id": "dNdN9AtVVH2J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def generate_marketing_email(product, tone, length):\n",
        "    return f\"Subject: Amazing {product}!\\n\\n(Tone: {tone}, Length: {length})\\n\\nDear Customer, buy our {product}...\"\n",
        "\n",
        "with gr.Blocks(theme=\"glass\") as dashboard_demo:\n",
        "\n",
        "    gr.Markdown(\"# üìß Marketing Email Generator\")\n",
        "\n",
        "    with gr.Row():\n",
        "\n",
        "        # --- LEFT SIDEBAR (Controls) ---\n",
        "        # scale=1 means \"Take up 20% of the width\"\n",
        "        with gr.Column(scale=1, min_width=300):\n",
        "            gr.Markdown(\"### ‚öôÔ∏è Settings\")\n",
        "            inp_product = gr.Textbox(label=\"Product Name\")\n",
        "            inp_tone = gr.Dropdown([\"Professional\", \"Funny\", \"Aggressive\"], label=\"Tone\", value=\"Professional\")\n",
        "            inp_length = gr.Slider(minimum=50, maximum=500, label=\"Word Count\", value=100)\n",
        "            btn_generate = gr.Button(\"Generate Email\", variant=\"primary\")\n",
        "\n",
        "        # --- RIGHT MAIN AREA (Result) ---\n",
        "        # scale=4 means \"Take up 80% of the width\"\n",
        "        with gr.Column(scale=4):\n",
        "            gr.Markdown(\"### üìÑ Result\")\n",
        "            out_result = gr.Textbox(label=\"Generated Email\", lines=10, show_copy_button=True)\n",
        "\n",
        "    # Wiring\n",
        "    btn_generate.click(\n",
        "        fn=generate_marketing_email,\n",
        "        inputs=[inp_product, inp_tone, inp_length],\n",
        "        outputs=out_result\n",
        "    )\n",
        "\n",
        "dashboard_demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "xUZHJOu_VSP1",
        "outputId": "2f55735e-d449-4dd2-c28c-ac7776a2e852"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-145012022.py:6: DeprecationWarning: The 'theme' parameter in the Blocks constructor will be removed in Gradio 6.0. You will need to pass 'theme' to Blocks.launch() instead.\n",
            "  with gr.Blocks(theme=\"glass\") as dashboard_demo:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://588426c199613f0958.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://588426c199613f0958.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://588426c199613f0958.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Design 2: The \"Swiss Army Knife\" Layout (Tabs)"
      ],
      "metadata": {
        "id": "JGbZVr_XVnmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Case:** When you have one app that does multiple unrelated things (e.g., \"Summarizer\", \"Translator\", \"Chat\"). You don't want to clutter the screen, so you hide them behind Tabs.\n",
        "\n",
        "**Key Concept:** `with gr.Tab(\"Name\"):`."
      ],
      "metadata": {
        "id": "aLz70aJ5Vp-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def summarize(text): return f\"Summary: {text[:20]}...\"\n",
        "def translate(text): return f\"Hola: {text}\"\n",
        "\n",
        "with gr.Blocks() as tab_demo:\n",
        "\n",
        "    gr.Markdown(\"# üõ†Ô∏è The AI Toolkit\")\n",
        "\n",
        "    # --- TAB 1: SUMMARIZER ---\n",
        "    with gr.Tab(\"üìù Summarizer\"):\n",
        "        gr.Markdown(\"Paste long text here to get a summary.\")\n",
        "        t1_input = gr.Textbox(lines=5, label=\"Long Text\")\n",
        "        t1_btn = gr.Button(\"Summarize\")\n",
        "        t1_output = gr.Textbox(label=\"Summary\")\n",
        "        t1_btn.click(summarize, t1_input, t1_output)\n",
        "\n",
        "    # --- TAB 2: TRANSLATOR ---\n",
        "    with gr.Tab(\"üåç Translator\"):\n",
        "        gr.Markdown(\"Translate English to Spanish.\")\n",
        "        with gr.Row():\n",
        "            t2_input = gr.Textbox(label=\"English\")\n",
        "            t2_output = gr.Textbox(label=\"Spanish\")\n",
        "        t2_btn = gr.Button(\"Translate\")\n",
        "        t2_btn.click(translate, t2_input, t2_output)\n",
        "\n",
        "    # --- TAB 3: SETTINGS ---\n",
        "    with gr.Tab(\"‚öôÔ∏è Config\"):\n",
        "        gr.Markdown(\"Nothing to see here yet!\")\n",
        "\n",
        "tab_demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "FrqS46F7Vsr7",
        "outputId": "97ade4ff-3bda-47f4-ad02-cdc53b2c82f4"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://84c1b480208d0f8427.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://84c1b480208d0f8427.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://84c1b480208d0f8427.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Design 3: The \"Wizard\" Layout (Accordion / Progressive)"
      ],
      "metadata": {
        "id": "zUdEvF3iV7_K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Use Case:** Complex workflows where you don't want to overwhelm the user. You show Step 1, then they open Step 2, etc. Or you hide advanced technical details (like JSON outputs) inside a collapsible box.\n",
        "\n",
        "**Key Concept:** `gr.Accordion(\"Title\", open=False)`."
      ],
      "metadata": {
        "id": "osmRwE5lV_TO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "\n",
        "def analyze_logic(text):\n",
        "    return \"Positive\", {\"score\": 0.98, \"sentiment\": \"happy\"}\n",
        "\n",
        "with gr.Blocks() as accordion_demo:\n",
        "\n",
        "    gr.Markdown(\"# üßô‚Äç‚ôÇÔ∏è Sentiment Wizard\")\n",
        "\n",
        "    # ALWAYS VISIBLE\n",
        "    user_text = gr.Textbox(label=\"Enter a sentence\")\n",
        "    analyze_btn = gr.Button(\"Analyze Sentiment\")\n",
        "\n",
        "    # OUTPUT 1: Simple Result\n",
        "    lbl_result = gr.Label(label=\"Top Sentiment\")\n",
        "\n",
        "    # OUTPUT 2: Advanced Details (Hidden by default)\n",
        "    # We use open=False so it starts closed\n",
        "    with gr.Accordion(\"See Advanced JSON Details\", open=False):\n",
        "        json_output = gr.JSON(label=\"Raw Data\")\n",
        "\n",
        "    analyze_btn.click(analyze_logic, user_text, [lbl_result, json_output])\n",
        "\n",
        "accordion_demo.launch(debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 680
        },
        "id": "6vsTGblLWBnp",
        "outputId": "4140fbb9-02ef-4bd7-8e5c-b12b7176e934"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted Jupyter notebook, which requires `share=True`. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://87640a1b532429710e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://87640a1b532429710e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7861 <> https://87640a1b532429710e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### üß† Which one should you use?"
      ],
      "metadata": {
        "id": "f4iP4d9EWRdu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "*   **Design 1 (Dashboard):** Use this for your \"Main Project\" in the course if you are building a tool (like a Resume Generator).\n",
        "*   **Design 2 (Tabs):** Use this if you are combining Week 1 (Chat) and Week 2 (RAG) into a single portfolio app.\n",
        "*   **Design 3 (Accordion):** Use this when you want to show \"Debug Info\" or \"Thinking Process\" (like the Chain-of-Thought from DeepSeek) without cluttering the main view."
      ],
      "metadata": {
        "id": "qeBeWJcLWPfV"
      }
    }
  ]
}