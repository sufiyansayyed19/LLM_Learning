{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMBrDGyx3oCDtxdemyjdHdy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufiyansayyed19/LLM_Learning/blob/main/W1Day5_1_Business_Brochure_Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a98030af-fcd1-4d63-a36e-38ba053498fa"
      },
      "source": [
        "### BUSINESS CHALLENGE:\n",
        "\n",
        "Create a product that builds a Brochure for a company to be used for prospective clients, investors and potential recruits.\n",
        "\n",
        "We will be provided a company name and their primary website.\n",
        "\n",
        "See the end of this notebook for examples of real-world business applications.\n",
        "\n",
        "And remember: I'm always available if you have problems or ideas! Please do reach out."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "All imports  \n",
        "import os  \n",
        "import json  \n",
        "from IPython.display import Markdown, display, update_display  \n",
        "from google.colab import userdata  \n",
        "from openai import OpenAI   "
      ],
      "metadata": {
        "id": "XOuPcjxNH2UX"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b71dc62"
      },
      "source": [
        "## 1.Scraper Functions\n",
        "\n",
        "This section defines functions to scrape website content and extract links. These functions are crucial for gathering the necessary information to create the brochure."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get website content"
      ],
      "metadata": {
        "id": "bRe2J2X79cgj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Install necessary libraries\n",
        "!pip install beautifulsoup4 requests markdownify\n",
        "\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from urllib.parse import urljoin\n",
        "\n",
        "# --- THESE ARE THE FUNCTIONS ED HAS IN 'scraper.py' ---\n",
        "\n",
        "def fetch_website_contents(url):\n",
        "    \"\"\"\n",
        "    Fetches the text content of a website, stripping out scripts and styles.\n",
        "    \"\"\"\n",
        "    print(f\"Scraping: {url}...\")\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        # Remove script and style elements\n",
        "        for script in soup([\"script\", \"style\", \"nav\", \"footer\"]):\n",
        "            script.extract()\n",
        "\n",
        "        # Get text\n",
        "        text = soup.get_text()\n",
        "\n",
        "        # Break into lines and remove leading/trailing space on each\n",
        "        lines = (line.strip() for line in text.splitlines())\n",
        "        # Break multi-headlines into a line each\n",
        "        chunks = (phrase.strip() for line in lines for phrase in line.split(\"  \"))\n",
        "        # Drop blank lines\n",
        "        text = '\\n'.join(chunk for chunk in chunks if chunk)\n",
        "\n",
        "        return text[:5000] # Limit to 5000 chars to save tokens\n",
        "    except Exception as e:\n",
        "        return f\"Error fetching {url}: {e}\"\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rmDY2JYsZFP",
        "outputId": "f536bf08-d74c-4021-8765-981751d9a7a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Collecting markdownify\n",
            "  Downloading markdownify-1.2.2-py3-none-any.whl.metadata (9.9 kB)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: six<2,>=1.15 in /usr/local/lib/python3.12/dist-packages (from markdownify) (1.17.0)\n",
            "Downloading markdownify-1.2.2-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: markdownify\n",
            "Successfully installed markdownify-1.2.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get website links"
      ],
      "metadata": {
        "id": "3ZgcBfa09n5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_website_links(url):\n",
        "    \"\"\"\n",
        "    Fetches all links from a website and converts relative links to absolute.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "        links = []\n",
        "        for a_tag in soup.find_all('a', href=True):\n",
        "            href = a_tag['href']\n",
        "            # Convert relative links (e.g. \"/about\") to full links\n",
        "            full_url = urljoin(url, href)\n",
        "            if full_url.startswith('http'):\n",
        "                links.append(full_url)\n",
        "\n",
        "        # Remove duplicates\n",
        "        return list(set(links))\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching links: {e}\")\n",
        "        return []\n",
        "\n",
        "print(\"Scraper functions loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_4BOmw4CGie",
        "outputId": "b2b83385-6bbe-47b7-ee39-6e8683bcb470"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scraper functions loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.API Key Set Up for colab"
      ],
      "metadata": {
        "id": "Oo2bSfjWCs3P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ADD key value in secret and check with code below"
      ],
      "metadata": {
        "id": "5efi86FbDE1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "try:\n",
        "    key = userdata.get('OPENAI_API_KEY')\n",
        "    print(f\"Success! Key found. It starts with: {key[:8]}...\")\n",
        "except Exception as e:\n",
        "    print(\"Error: Could not find key. Did you turn the toggle switch ON?\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9UsV4rvCmzl",
        "outputId": "be2e5271-7add-4cd5-9f5c-c36109d2727a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Key found. It starts with: sk-or-v1...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.API Call Setup"
      ],
      "metadata": {
        "id": "ZFqTdU0cCmDK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        " #1. Setup API Key (Make sure you added OPENAI_API_KEY in Colab Secrets)\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# 2. Setup Client (OpenRouter)\n",
        "client = OpenAI(\n",
        "    base_url=\"https://openrouter.ai/api/v1\",\n",
        "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
        ")\n",
        "\n",
        "# 3. Define Models\n",
        "# We use a cheap, fast model for link selection\n",
        "LINK_MODEL = \"openai/gpt-4o-mini\"\n",
        "# We use a smarter model for writing the brochure\n",
        "WRITER_MODEL = \"openai/gpt-4o-mini\"\n",
        "\n",
        "print(\"Client and Models configured!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3T7JqqcQDSNl",
        "outputId": "840209d6-d5b9-429a-dec8-2dcedc590cf3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Client and Models configured!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Prompt Design"
      ],
      "metadata": {
        "id": "j1fG7HjvD7Ss"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### i-System Prompts"
      ],
      "metadata": {
        "id": "T5NGVF2xEuIX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Link system prompt"
      ],
      "metadata": {
        "id": "pbOqTLRwEGL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # --- SYSTEM PROMPTS ---\n",
        "# link_system_prompt = \"\"\"\n",
        "# You are provided with a list of links found on a webpage.\n",
        "# You are able to decide which of the links would be most relevant to include in a brochure about the company,\n",
        "# such as links to an About page, or a Company page, or Careers/Jobs pages.\n",
        "# You should respond in JSON as in this example:\n",
        "# {\n",
        "#     \"links\": [\n",
        "#         {\"type\": \"about page\", \"url\": \"https://full.url/goes/here/about\"},\n",
        "#         {\"type\": \"careers page\", \"url\": \"https://another.full.url/careers\"}\n",
        "#     ]\n",
        "# }\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "v1sS1VNKELI3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- SYSTEM PROMPTS ---\n",
        "link_system_prompt = \"\"\"\n",
        "You are provided with a list of links found on a portfoio webpage.\n",
        "You are able to decide which of the links would be most relevant to generate the one page attractive CV,\n",
        "{\n",
        "    \"links\": [\n",
        "        {\"type\": \"About me\", \"url\": \"https://full.url/goes/here/about\"},\n",
        "        {\"type\": \"Projects\", \"url\": \"https://another.full.url/projets/..\"}\n",
        "    ]\n",
        "}\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "45XEQhNwUKWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Brochure System prompt"
      ],
      "metadata": {
        "id": "kdLVdcjkEKx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "brochure_system_prompt = \"\"\"\n",
        "You are an expert career assistant and resume writer.\n",
        "\n",
        "You will be provided with a portfolio website URL that contains multiple internal links (such as About, Projects, Experience, Skills, Blogs, GitHub, Case Studies, etc.).\n",
        "\n",
        "Your task is to:\n",
        "\n",
        "Analyze the entire portfolio, including all relevant linked pages\n",
        "\n",
        "Extract meaningful information about:\n",
        "\n",
        "Skills and technologies\n",
        "\n",
        "Projects and their impact\n",
        "\n",
        "Experience, internships, or freelancing work\n",
        "\n",
        "Education and certifications\n",
        "\n",
        "Open-source contributions or research (if any)\n",
        "\n",
        "Achievements and measurable results\n",
        "\n",
        "Infer strengths, specialization, and career direction based on evidence (not assumptions)\n",
        "\n",
        "Then generate a high-quality, ATS-friendly, and recruiter-attractive CV tailored for modern tech roles (e.g., Software Engineer, ML Engineer, Data Scientist, AI Engineer, Full-Stack Developer ‚Äî depending on the portfolio content).\n",
        "\n",
        "CV Requirements\n",
        "\n",
        "Use strong action verbs and impact-driven bullet points\n",
        "\n",
        "Quantify results wherever possible (performance, scale, users, accuracy, efficiency, etc.)\n",
        "\n",
        "Highlight real projects over generic skills\n",
        "\n",
        "Reflect industry-level professionalism, not student-level wording\n",
        "\n",
        "Optimize for clarity, conciseness, and credibility\n",
        "\n",
        "Output Format (in Markdown)\n",
        "\n",
        "Professional Summary (2‚Äì3 lines, sharp and role-focused)\n",
        "\n",
        "Core Skills (grouped by category)\n",
        "\n",
        "Experience / Internships / Freelance (if applicable)\n",
        "\n",
        "Projects (most important section)\n",
        "\n",
        "Education\n",
        "\n",
        "Certifications / Achievements (if available)\n",
        "\n",
        "Optional: Publications, Blogs, Open Source\n",
        "\n",
        "Do not invent information.\n",
        "If something is missing, omit it gracefully.\n",
        "Focus on making the candidate look hire-ready and competitive.\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "r7yXHznrEWpw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ii- User Prompt"
      ],
      "metadata": {
        "id": "O2vtREAtEWEA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User prompt to select relevent links"
      ],
      "metadata": {
        "id": "Am-i2SZWFlMx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_links_user_prompt(url):\n",
        "#     raw_links = fetch_website_links(url)\n",
        "#     # Take only first 30 links to save tokens\n",
        "#     links_text = \"\\n\".join(raw_links[:30])\n",
        "#     user_prompt = f\"\"\"\n",
        "#     Here is the list of links on the website {url} -\n",
        "#     Please decide which of these are relevant web links for a brochure about the company,\n",
        "#     respond with the full https URL in JSON format.\n",
        "#     Do not include Terms of Service, Privacy, email links.\n",
        "\n",
        "#     Links:\n",
        "#     {links_text}\n",
        "#     \"\"\"\n",
        "#     return user_prompt"
      ],
      "metadata": {
        "id": "5KVwcXSSEmlV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_links_user_prompt(url):\n",
        "    raw_links = fetch_website_links(url)\n",
        "    # Take only first 30 links to save tokens\n",
        "    links_text = \"\\n\".join(raw_links[:30])\n",
        "    user_prompt = f\"\"\"\n",
        "    Here is the list of links on the website {url} -\n",
        "    Please decide which of these are relevant web links for a cv generated from webpage,\n",
        "    respond with the full https URL in JSON format.\n",
        "    Do not include Terms of Service, Privacy, email links.\n",
        "\n",
        "    Links:\n",
        "    {links_text}\n",
        "    \"\"\"\n",
        "    return user_prompt"
      ],
      "metadata": {
        "id": "wvAnRHrXVewk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### User prompt to Create brochure"
      ],
      "metadata": {
        "id": "ENvg4TX8Ft5V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def get_brochure_user_prompt(company_name, url):\n",
        "#     print(\"üì• Fetching website content (this takes a moment)...\")\n",
        "#     content = fetch_page_and_all_relevant_links(url)\n",
        "\n",
        "#     user_prompt = f\"\"\"\n",
        "#     You are looking at a company called: {company_name}\n",
        "#     Here are the contents of its landing page and other relevant pages;\n",
        "#     use this information to build a short brochure of the company in markdown without code blocks.\\n\\n\n",
        "#     {content[:10000]}\n",
        "#     \"\"\"\n",
        "#     # (Limited to 10k chars to ensure we don't overflow context)\n",
        "#     return user_prompt"
      ],
      "metadata": {
        "id": "yhK-YKyRFNe_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_cv_user_prompt(candidate_name, portfolio_url):\n",
        "    print(\"üì• Fetching portfolio content (this may take a moment)...\")\n",
        "    content = fetch_page_and_all_relevant_links(portfolio_url)\n",
        "\n",
        "    user_prompt = f\"\"\"\n",
        "You are an expert career assistant and professional resume writer.\n",
        "\n",
        "You are analyzing the **portfolio website** of a candidate named: {candidate_name}\n",
        "\n",
        "The portfolio contains multiple linked pages such as About, Projects, Skills, Experience,\n",
        "Blogs, GitHub, Case Studies, or similar sections.\n",
        "\n",
        "Your task is to:\n",
        "- Analyze the entire portfolio content provided below\n",
        "- Extract verified information about:\n",
        "  - Technical skills and tools\n",
        "  - Projects and their real-world impact\n",
        "  - Internships, freelance work, or professional experience\n",
        "  - Education and certifications\n",
        "  - Open-source contributions, research, or blogs (if present)\n",
        "- Infer strengths and specialization ONLY from the given evidence\n",
        "\n",
        "Then generate a **high-quality, ATS-friendly, recruiter-ready CV** in **markdown format**\n",
        "(without code blocks).\n",
        "\n",
        "### CV Writing Guidelines\n",
        "- Use strong action verbs and impact-focused bullet points\n",
        "- Quantify results where possible (accuracy, performance, scale, users, revenue, etc.)\n",
        "- Emphasize real projects over generic skill lists\n",
        "- Maintain industry-level, professional language\n",
        "- Do NOT invent or assume missing information\n",
        "\n",
        "### CV Structure\n",
        "- Professional Summary (2‚Äì3 concise lines)\n",
        "- Core Skills (grouped by category)\n",
        "- Experience / Internships / Freelance (if available)\n",
        "- Projects (most important section)\n",
        "- Education\n",
        "- Certifications / Achievements (if available)\n",
        "- Optional: Publications, Blogs, Open Source\n",
        "\n",
        "Here is the portfolio content to analyze:\n",
        "\n",
        "{content[:10000]}\n",
        "\"\"\"\n",
        "\n",
        "    # Limited to 10k characters to avoid context overflow\n",
        "    return user_prompt\n"
      ],
      "metadata": {
        "id": "IvQj6JEmVjm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Content Get functions"
      ],
      "metadata": {
        "id": "yxn58MNXF628"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Page and all Links function"
      ],
      "metadata": {
        "id": "yEtZODgdGkdu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_page_and_all_relevant_links(url):\n",
        "    # 1. Get Main Page\n",
        "    contents = fetch_website_contents(url)\n",
        "\n",
        "    # 2. Get Relevant Links\n",
        "    relevant_links = select_relevant_links(url)\n",
        "\n",
        "    result = f\"## Landing Page:\\n\\n{contents}\\n## Relevant Links:\\n\"\n",
        "\n",
        "    # 3. Get Content of Relevant Links\n",
        "    for link in relevant_links['links']:\n",
        "        print(f\"   Reading: {link['type']} ({link['url']})\")\n",
        "        link_content = fetch_website_contents(link[\"url\"])\n",
        "        result += f\"\\n\\n### Link: {link['type']}\\n{link_content}\"\n",
        "\n",
        "    return result"
      ],
      "metadata": {
        "id": "h2VNR8AwFLQr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Select relevant links from all links"
      ],
      "metadata": {
        "id": "Y1-x1-hrGuot"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "def select_relevant_links(url):\n",
        "    print(f\"üîç Analyzing links for {url}...\")\n",
        "    response = client.chat.completions.create(\n",
        "        model=LINK_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": link_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": get_links_user_prompt(url)}\n",
        "        ],\n",
        "        response_format={\"type\": \"json_object\"}\n",
        "    )\n",
        "    result = response.choices[0].message.content\n",
        "    try:\n",
        "        links = json.loads(result)\n",
        "        print(f\"‚úÖ Found {len(links['links'])} relevant links.\")\n",
        "        return links\n",
        "    except:\n",
        "        print(\"Error parsing JSON\")\n",
        "        return {\"links\": []}"
      ],
      "metadata": {
        "id": "xiZFm_ukE-i5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6.Brochure Generator function"
      ],
      "metadata": {
        "id": "GwnCMvLqG8Gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Markdown, display, update_display\n",
        "\n",
        "def stream_brochure(company_name, url):\n",
        "    print(f\"üöÄ Generating brochure for {company_name}...\")\n",
        "    stream = client.chat.completions.create(\n",
        "        model=WRITER_MODEL,\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": brochure_system_prompt},\n",
        "            {\"role\": \"user\", \"content\": get_brochure_user_prompt(company_name, url)}\n",
        "          ],\n",
        "        stream=True\n",
        "    )\n",
        "    response = \"\"\n",
        "    display_handle = display(Markdown(\"Wait for it...\"), display_id=True)\n",
        "    for chunk in stream:\n",
        "        content = chunk.choices[0].delta.content or ''\n",
        "        response += content\n",
        "        update_display(Markdown(response), display_id=display_handle.display_id)"
      ],
      "metadata": {
        "id": "LUr7JaT5FQZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.Driver"
      ],
      "metadata": {
        "id": "vobZ53M1HDIb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- RUN IT! ---\n",
        "# Use standard HuggingFace URL\n",
        "stream_brochure(\"Sufiyan Portfolio Site\", \"https://sufiyan-sayyed.vercel.app/\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fJP_v1QKFSXD",
        "outputId": "305e44af-1a9b-4b03-b1be-04ee51529d42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Generating brochure for HuggingFace...\n",
            "üì• Fetching website content (this takes a moment)...\n",
            "Scraping: https://huggingface.co...\n",
            "üîç Analyzing links for https://huggingface.co...\n",
            "‚úÖ Found 4 relevant links.\n",
            "   Reading: about page (https://huggingface.co/docs)\n",
            "Scraping: https://huggingface.co/docs...\n",
            "   Reading: blog page (https://huggingface.co/blog)\n",
            "Scraping: https://huggingface.co/blog...\n",
            "   Reading: careers page (https://huggingface.co/enterprise)\n",
            "Scraping: https://huggingface.co/enterprise...\n",
            "   Reading: pricing page (https://huggingface.co/pricing)\n",
            "Scraping: https://huggingface.co/pricing...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Hugging Face Brochure\n\n## Overview\n**Hugging Face** is a leading platform for the machine learning community dedicated to building the future of AI. Through collaboration, innovation, and open-source values, we empower users to create, discover, and collaborate on machine learning models, datasets, and applications.\n\n## Our Mission\nAt Hugging Face, we believe in the potential of artificial intelligence to transform our world. We serve as the home of machine learning, providing tools, resources, and a vibrant community for anyone interested in AI.\n\n## Key Offerings\n- **Collaborative Platform**: Host, discover, and collaborate on over **2 million models, applications,** and **500k datasets**.\n- **Open Source**: We contribute to the foundation of ML tooling with open-source libraries like Transformers, Diffusers, and Tokenizers.\n- **Enterprise Solutions**: Tailored offerings for organizations to accelerate AI development with robust security and dedicated support.\n\n## Who We Serve\nOver **50,000 organizations** use Hugging Face, including industry leaders like Google, Microsoft, Amazon, and Intel. Our user base spans diverse sectors including tech, education, and non-profits, all leveraging our platform for sustainable AI development.\n\n## Company Culture\nWe pride ourselves on a **community-driven culture** that emphasizes collaboration and inclusivity. Our team fosters an environment that encourages innovation and promotes shared learning among developers, researchers, and enthusiasts in the AI space.\n\n### Core Values\n- **Collaboration**: We thrive on the active participation of our users and contributors.\n- **Innovation**: Continuous learning and pioneering new technologies is at the core of our mission.\n- **Transparency**: Open-source accessibility ensures everyone can contribute and benefit from our tools.\n\n## Join Us\n### Careers at Hugging Face\nAt Hugging Face, we are always looking for passionate individuals to join our team of experts and innovators. We offer a range of opportunities across various domains in AI development, engineering, and community support.\n\n**Why Work With Us?**\n- Be a part of a passionate and collaborative team.\n- Work on cutting-edge technology that shapes the future of AI.\n- Engage with a vibrant community of developers and researchers.\n\nExplore our current job openings and find your place in the AI revolution.\n\n## Get Involved\nJoin our community and explore how you can utilize our resources, participate in discussions, and contribute to the ever-growing field of machine learning. Whether you're a seasoned expert or a curious beginner, Hugging Face welcomes you to embark on this journey with us.\n\n**Discover more at**: [Hugging Face Website](https://huggingface.co)  \n**Connect with us**: [Join our Discord](https://huggingface.co/discord)  |  [Read our Blog](https://huggingface.co/blog)\n\n---\n\nWelcome to Hugging Face, where together we are building a brighter future through AI."
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1f6578a"
      },
      "source": [
        "## Function Call Flow for `stream_brochure`\n",
        "\n",
        "The `stream_brochure` function orchestrates the entire brochure generation process by calling several other functions. Here's the sequence of execution:\n",
        "\n",
        "1.  **`stream_brochure(company_name, url)`**:\n",
        "    *   Initiates the process and prints a message indicating brochure generation has started.\n",
        "    *   Internally calls `get_brochure_user_prompt(company_name, url)` to prepare the prompt for the brochure writer model.\n",
        "    *   Sends this prompt to the `WRITER_MODEL` via `client.chat.completions.create` and streams the response to the display.\n",
        "\n",
        "2.  **`get_brochure_user_prompt(company_name, url)`**:\n",
        "    *   Prints a message about fetching website content.\n",
        "    *   Its primary role is to gather all necessary content by calling `fetch_page_and_all_relevant_links(url)`.\n",
        "    *   Formats the retrieved content into a user prompt for the brochure writer.\n",
        "\n",
        "3.  **`fetch_page_and_all_relevant_links(url)`**:\n",
        "    *   First, it fetches the content of the main URL by calling `fetch_website_contents(url)`.\n",
        "    *   Then, it identifies relevant links on the website by calling `select_relevant_links(url)`.\n",
        "    *   For each relevant link found, it prints which link type is being read and then calls `fetch_website_contents(link[\"url\"])` again to get the content of that specific linked page.\n",
        "    *   Aggregates all fetched content into a single string.\n",
        "\n",
        "4.  **`select_relevant_links(url)`**:\n",
        "    *   Prints a message indicating link analysis is in progress.\n",
        "    *   It prepares a prompt for the `LINK_MODEL` by calling `get_links_user_prompt(url)`.\n",
        "    *   Sends this prompt to the `LINK_MODEL` via `client.chat.completions.create` to get a JSON response of relevant links.\n",
        "    *   Parses the JSON response and returns a list of relevant links.\n",
        "\n",
        "5.  **`get_links_user_prompt(url)`**:\n",
        "    *   Calls `fetch_website_links(url)` to get all raw links from the given URL.\n",
        "    *   Filters and formats these links into a user prompt for the `LINK_MODEL`.\n",
        "\n",
        "6.  **`fetch_website_links(url)`**:\n",
        "    *   Uses `requests` and `BeautifulSoup` to scrape all `<a>` tags from the specified URL.\n",
        "    *   Converts relative URLs to absolute URLs and returns a unique list of all found links.\n",
        "\n",
        "7.  **`fetch_website_contents(url)`**:\n",
        "    *   Prints the URL being scraped.\n",
        "    *   Uses `requests` and `BeautifulSoup` to fetch the HTML content of the URL.\n",
        "    *   Strips out script, style, navigation, and footer elements.\n",
        "    *   Extracts and cleans the visible text content, limiting it to the first 5000 characters to save tokens."
      ]
    }
  ]
}