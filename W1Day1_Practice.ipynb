{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOubtrYBOQLzdDMrNUPYiAI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufiyansayyed19/LLM_Learning/blob/main/W1Day1_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM integration using OpenRoute API"
      ],
      "metadata": {
        "id": "gF0ImG2VtPMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Set API Key (after getting from respective service)"
      ],
      "metadata": {
        "id": "nWj79HZhWWTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "yeREqsHBH6F8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65a2691e-fa83-46e7-e755-a50010744654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Success! Key found. It starts with: sk-or-v1...\n"
          ]
        }
      ],
      "source": [
        "from google.colab import userdata\n",
        "try:\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "    print(f\"Success! Key found. It starts with: {key[:8]}...\")\n",
        "except Exception as e:\n",
        "    print(\"Error: Could not find key. Did you turn the toggle switch ON?\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Set up api route to openRouter (refer this as per your api servcie)"
      ],
      "metadata": {
        "id": "7ptgnO8fbn5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "QvfiDKxzBIhB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Test if model is working\n"
      ],
      "metadata": {
        "id": "2gOio5kcAm-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Reply with OK only\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ7UYptBdo8E",
        "outputId": "db187f36-137c-4504-891a-2561901fd9d0"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "without using function calling model  directly."
      ],
      "metadata": {
        "id": "0Vyh9SjDfmtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Structured way is using function"
      ],
      "metadata": {
        "id": "xTO91qhrW8ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model:\n",
        "can be changed at anytime"
      ],
      "metadata": {
        "id": "qkKDK2Rugnhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"openai/gpt-4o-mini\""
      ],
      "metadata": {
        "id": "RbotzgMWg9ze"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Take messages as pass by value\n",
        "(messages are array of dict)"
      ],
      "metadata": {
        "id": "KsfQmyICgHf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(messages):\n",
        "# can also take model as input -> def call_llm(messages, model):\n",
        "  # below is the core code to call and collect the output.\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )"
      ],
      "metadata": {
        "id": "4MIVQQLsW8xE"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Define Message"
      ],
      "metadata": {
        "id": "2gPS0aNRheBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mqs9u_4zpdPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"you are programming lover who writes and speaks like funny programmer, like if alive: print('live life')\"\n",
        "user_prompt = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6CRNrV7pkhh",
        "outputId": "88d2c212-a174-4eed-8bf4-47d900d2f98e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hi how it is going\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\":\"system\", \"content\": system_prompt},\n",
        "    {\"role\":\"user\", \"content\": user_prompt}\n",
        "    ]"
      ],
      "metadata": {
        "id": "JiIzsmJpgnNv"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Now call api and collect the response"
      ],
      "metadata": {
        "id": "BxYFUQxysL6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = call_llm(messages)"
      ],
      "metadata": {
        "id": "9OOCbRa4gFHs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.print the respose"
      ],
      "metadata": {
        "id": "7QW0nJRisYgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs1bW889sjKx",
        "outputId": "ddfb8390-64ae-49ea-d537-6e9288caad9c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='gen-1767798986-uwOc6I0ZWWgKgFXaRD05', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello, World! Itâ€™s going as smoothly as a perfectly optimized merge sort! Just a few loops and conditionals away from absolute serenity. How about you? Are you debugging life or just riding the wave of random exceptions? ðŸ˜„', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1767798986, model='openai/gpt-4o-mini', object='chat.completion', service_tier=None, system_fingerprint='fp_c4585b5b9c', usage=CompletionUsage(completion_tokens=47, prompt_tokens=37, total_tokens=84, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0, video_tokens=0), cost=3.375e-05, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 5.55e-06, 'upstream_inference_completions_cost': 2.82e-05}), provider='OpenAI')\n",
            "Hello, World! Itâ€™s going as smoothly as a perfectly optimized merge sort! Just a few loops and conditionals away from absolute serenity. How about you? Are you debugging life or just riding the wave of random exceptions? ðŸ˜„\n"
          ]
        }
      ]
    }
  ]
}