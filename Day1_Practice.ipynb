{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyMZG+px1IXZUtDu+28RjrvP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sufiyansayyed19/LLM_Learning/blob/main/Day1_Practice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM integration using OpenRoute API"
      ],
      "metadata": {
        "id": "gF0ImG2VtPMo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.Set API Key (after getting from respective service)"
      ],
      "metadata": {
        "id": "nWj79HZhWWTk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "yeREqsHBH6F8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "api_key = os.getenv(\"OPENAI_API_KEY\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check if API key is set"
      ],
      "metadata": {
        "id": "yxYOpEslWraR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if not api_key:\n",
        "  raise ValueError(\"OPENAI_KEY not set\")\n",
        "else:\n",
        "  print(\"API key detected\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f26-hihjWWyJ",
        "outputId": "eb3d86fa-a070-4f36-fe28-5aaf48a0c06f"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "API key detected\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.Set up api route to openRouter (refer this as per your api servcie)"
      ],
      "metadata": {
        "id": "7ptgnO8fbn5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "client = OpenAI(\n",
        "    api_key=api_key,\n",
        "    base_url=\"https://openrouter.ai/api/v1\"\n",
        ")"
      ],
      "metadata": {
        "id": "QvfiDKxzBIhB"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.Test if model is working\n"
      ],
      "metadata": {
        "id": "2gOio5kcAm-h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = client.chat.completions.create(\n",
        "    model=\"openai/gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"user\", \"content\": \"Reply with OK only\"}\n",
        "    ]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZ7UYptBdo8E",
        "outputId": "e5ada652-bc88-41f3-b512-66f22da4bf7d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "without using function calling model  directly."
      ],
      "metadata": {
        "id": "0Vyh9SjDfmtA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.Structured way is using function"
      ],
      "metadata": {
        "id": "xTO91qhrW8ZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define model:\n",
        "can be changed at anytime"
      ],
      "metadata": {
        "id": "qkKDK2Rugnhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model=\"openai/gpt-4o-mini\""
      ],
      "metadata": {
        "id": "RbotzgMWg9ze"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Take messages as pass by value\n",
        "(messages are array of dict)"
      ],
      "metadata": {
        "id": "KsfQmyICgHf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def call_llm(messages):\n",
        "# can also take model as input -> def call_llm(messages, model):\n",
        "  # below is the core code to call and collect the output.\n",
        "    return client.chat.completions.create(\n",
        "        model=model,\n",
        "        messages=messages\n",
        "    )"
      ],
      "metadata": {
        "id": "4MIVQQLsW8xE"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5.Define Message"
      ],
      "metadata": {
        "id": "2gPS0aNRheBm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Mqs9u_4zpdPx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = \"you are programming lover who writes and speaks like funny programmer, like if alive: print('live life')\"\n",
        "user_prompt = input()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W6CRNrV7pkhh",
        "outputId": "fee8501b-e913-4427-9afe-53fca1a18cc5"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "give me a good joke \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\"role\":\"system\", \"content\": system_prompt},\n",
        "    {\"role\":\"user\", \"content\": user_prompt}\n",
        "    ]"
      ],
      "metadata": {
        "id": "JiIzsmJpgnNv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6.Now call api and collect the response"
      ],
      "metadata": {
        "id": "BxYFUQxysL6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = call_llm(messages)"
      ],
      "metadata": {
        "id": "9OOCbRa4gFHs"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7.print the respose"
      ],
      "metadata": {
        "id": "7QW0nJRisYgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fs1bW889sjKx",
        "outputId": "385afb03-540c-4bec-df63-2e67c45b7d9f"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ChatCompletion(id='gen-1767290213-7B5GXsMefjS1QsqBCPb3', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Why do programmers prefer dark mode?\\n\\nBecause light attracts bugs! üêõüí°', refusal=None, role='assistant', annotations=None, audio=None, function_call=None, tool_calls=None, reasoning=None), native_finish_reason='stop')], created=1767290213, model='openai/gpt-4o-mini', object='chat.completion', service_tier=None, system_fingerprint='fp_29330a9688', usage=CompletionUsage(completion_tokens=17, prompt_tokens=38, total_tokens=55, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, image_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0, video_tokens=0), cost=1.59e-05, is_byok=False, cost_details={'upstream_inference_cost': None, 'upstream_inference_prompt_cost': 5.7e-06, 'upstream_inference_completions_cost': 1.02e-05}), provider='OpenAI')\n",
            "Why do programmers prefer dark mode?\n",
            "\n",
            "Because light attracts bugs! üêõüí°\n"
          ]
        }
      ]
    }
  ]
}